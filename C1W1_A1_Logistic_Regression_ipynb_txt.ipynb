{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/NLP/blob/main/C1W1_A1_Logistic_Regression_ipynb_txt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install twython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK3GL210MLmS",
        "outputId": "62b2f92b-c3a7-434f-c46a-11131b7b0443"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting twython\n",
            "  Downloading twython-3.9.1-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from twython) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from twython) (1.3.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.4.0->twython) (3.2.0)\n",
            "Installing collected packages: twython\n",
            "Successfully installed twython-3.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://raw.githubusercontent.com/amanchadha/coursera-natural-language-processing-specialization/master/1%20-%20Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces/Week%201/utils.py'\n",
        "!wget 'https://raw.githubusercontent.com/amanchadha/coursera-natural-language-processing-specialization/master/1%20-%20Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces/Week%201/logistic_features.csv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "465sL9J37oaU",
        "outputId": "0c2c200f-6f7d-42b2-a9ed-f91a0f4d6814"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-22 06:23:14--  https://raw.githubusercontent.com/amanchadha/coursera-natural-language-processing-specialization/master/1%20-%20Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces/Week%201/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2261 (2.2K) [text/plain]\n",
            "Saving to: ‘utils.py’\n",
            "\n",
            "\rutils.py              0%[                    ]       0  --.-KB/s               \rutils.py            100%[===================>]   2.21K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-09-22 06:23:14 (32.7 MB/s) - ‘utils.py’ saved [2261/2261]\n",
            "\n",
            "--2022-09-22 06:23:14--  https://raw.githubusercontent.com/amanchadha/coursera-natural-language-processing-specialization/master/1%20-%20Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces/Week%201/logistic_features.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 162402 (159K) [text/plain]\n",
            "Saving to: ‘logistic_features.csv’\n",
            "\n",
            "logistic_features.c 100%[===================>] 158.60K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-09-22 06:23:15 (5.33 MB/s) - ‘logistic_features.csv’ saved [162402/162402]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVFcUHWb7SIe"
      },
      "source": [
        "# Assignment 1: Logistic Regression\n",
        "Welcome to week one of this specialization. You will learn about logistic regression. Concretely, you will be implementing logistic regression for sentiment analysis on tweets. Given a tweet, you will decide if it has a positive sentiment or a negative one. Specifically you will: \n",
        "\n",
        "* Learn how to extract features for logistic regression given some text\n",
        "* Implement logistic regression from scratch\n",
        "* Apply logistic regression on a natural language processing task\n",
        "* Test using your logistic regression\n",
        "* Perform error analysis\n",
        "\n",
        "We will be using a data set of tweets. Hopefully you will get more than 99% accuracy.  \n",
        "Run the cell below to load in the packages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fcBT-2-7SIj"
      },
      "source": [
        "## Import functions and data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4ynCcBm17SIk"
      },
      "outputs": [],
      "source": [
        "# run this cell to import nltk\n",
        "import nltk\n",
        "from os import getcwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iSeB6F27SIm"
      },
      "source": [
        "### Imported functions\n",
        "\n",
        "Download the data needed for this assignment. Check out the [documentation for the twitter_samples dataset](http://www.nltk.org/howto/twitter.html).\n",
        "\n",
        "* twitter_samples: if you're running this notebook on your local computer, you will need to download it using:\n",
        "```Python\n",
        "nltk.download('twitter_samples')\n",
        "```\n",
        "\n",
        "* stopwords: if you're running this notebook on your local computer, you will need to download it using:\n",
        "```python\n",
        "nltk.download('stopwords')\n",
        "```\n",
        "\n",
        "#### Import some helper functions that we provided in the utils.py file:\n",
        "* `process_tweet()`: cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\n",
        "* `build_freqs()`: this counts how often a word in the 'corpus' (the entire set of tweets) was associated with a positive label '1' or a negative label '0', then builds the `freqs` dictionary, where each key is a (word,label) tuple, and the value is the count of its frequency within the corpus of tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "h6cEiEAw7SIp"
      },
      "outputs": [],
      "source": [
        "# add folder, tmp2, from our local workspace containing pre-downloaded corpora files to nltk's data path\n",
        "# this enables importing of these files without downloading it again when we refresh our workspace\n",
        "\n",
        "filePath = f\"{getcwd()}/../tmp2/\"\n",
        "nltk.data.path.append(filePath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "K7FMd8hX7SIq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import twitter_samples \n",
        "\n",
        "from utils import process_tweet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxGvEDiD7SIr"
      },
      "source": [
        "### Prepare the data\n",
        "* The `twitter_samples` contains subsets of 5,000 positive tweets, 5,000 negative tweets, and the full set of 10,000 tweets.  \n",
        "    * If you used all three datasets, we would introduce duplicates of the positive tweets and negative tweets.  \n",
        "    * You will select just the five thousand positive tweets and five thousand negative tweets."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQmVhi0rP6GV",
        "outputId": "8b2ceaca-e79e-4536-ac27-4d8a2559811c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FFxqoKKt7SIt"
      },
      "outputs": [],
      "source": [
        "# select the set of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(all_positive_tweets))\n",
        "print(len(all_negative_tweets))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CLBBa3RQ61V",
        "outputId": "c531e047-1ab2-413d-daae-71f63b94c3b0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000\n",
            "5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_positive_tweets[0],'\\n')\n",
        "print(all_negative_tweets[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtZMXG8qQDm4",
        "outputId": "f616e7f5-8182-466b-9a73-7ca5b25226a6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :) \n",
            "\n",
            "Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh8tZkDx7SIv"
      },
      "source": [
        "* Train test split: 20% will be in the test set, and 80% in the training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "t0NVNZ1q7SIw"
      },
      "outputs": [],
      "source": [
        "# split the data into two pieces, one for training and one for testing (validation set) \n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg \n",
        "test_x = test_pos + test_neg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fH-8usOU7SIx"
      },
      "source": [
        "* Create the numpy array of positive labels and negative labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iA5ePtqU7SIx"
      },
      "outputs": [],
      "source": [
        "# combine positive and negative labels\n",
        "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sA8tbdfm7SIy",
        "outputId": "5861c1ad-ccac-47c3-f373-3475311e3912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_y.shape = (8000, 1)\n",
            "test_y.shape = (2000, 1)\n"
          ]
        }
      ],
      "source": [
        "# Print the shape train and test sets\n",
        "print(\"train_y.shape = \" + str(train_y.shape))\n",
        "print(\"test_y.shape = \" + str(test_y.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90IeBDjy7SIz"
      },
      "source": [
        "* Create the frequency dictionary using the imported `build_freqs()` function.  \n",
        "cy dictionary that's being built. \n",
        "* The key is the tuple (word, label), such as (\"happy\",1) or (\"happy\",0).  The value stored for each key is the count of how many times the word \"happy\" was associated with a positive label, or how many times \"happy\" was associated with a negative label."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_freqs(tweets, ys):\n",
        "    \"\"\"Build frequencies.\n",
        "    Input:\n",
        "        tweets: a list of tweets\n",
        "        ys: an m x 1 array with the sentiment label of each tweet\n",
        "            (either 0 or 1)\n",
        "    Output:\n",
        "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
        "        frequency\n",
        "    \"\"\"\n",
        "    # Convert np array to list since zip needs an iterable.\n",
        "    # The squeeze is necessary or the list ends up with one element.\n",
        "    # Also note that this is just a NOP if ys is already a list.\n",
        "\n",
        "    yslist = np.squeeze(ys).tolist()\n",
        "\n",
        "    # Start with an empty dictionary and populate it by looping over all tweets\n",
        "    # and over all processed words in each tweet.\n",
        "    freqs = {}\n",
        "    for y,tweet in zip(yslist,tweets):\n",
        "      for word in process_tweet(tweet):\n",
        "        pair = (word,y)\n",
        "        if pair in freqs:\n",
        "          freqs[pair] += 1\n",
        "        else:\n",
        "          freqs[pair] = 1\n",
        "        \n",
        "    return freqs\n",
        "\n"
      ],
      "metadata": {
        "id": "8qf25oPORshs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8DdiSAU7SI0",
        "outputId": "fbf76ccb-53c5-48dc-b19b-e69a811b635d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type(freqs) = <class 'dict'>\n",
            "len(freqs) = 11338\n"
          ]
        }
      ],
      "source": [
        "# create frequency dictionary\n",
        "freqs = build_freqs(train_x, train_y)\n",
        "\n",
        "# check the output\n",
        "print(\"type(freqs) = \" + str(type(freqs)))\n",
        "print(\"len(freqs) = \" + str(len(freqs.keys())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2j1BwK17SI1"
      },
      "source": [
        "#### Expected output\n",
        "```\n",
        "type(freqs) = <class 'dict'>\n",
        "len(freqs) = 11346\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(freqs[('show', 1.0)])\n",
        "print(freqs[('show', 0)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3b-3x_Pd-QD",
        "outputId": "248c0f2d-a425-4cc9-bf33-fb99b5c7c26a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(freqs[('good', 1.0)])\n",
        "print(freqs[('good', 0)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68ECGxLLePIm",
        "outputId": "b779311e-787e-4bdf-ad76-b87fb77394d5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "191\n",
            "83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcS-9Ilq7SI3"
      },
      "source": [
        "### Process tweet\n",
        "The given function `process_tweet()` tokenizes the tweet into individual words, removes stop words and applies stemming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ygYw6Y97SI3",
        "outputId": "8ea0d7ec-f443-4b0c-f95e-2b5740c7e516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is an example of a positive tweet: \n",
            " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "\n",
            "This is an example of the processed version of the tweet: \n",
            " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
          ]
        }
      ],
      "source": [
        "# test the function below\n",
        "print('This is an example of a positive tweet: \\n', train_x[0])\n",
        "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCgNS5Ai7SI4"
      },
      "source": [
        "#### Expected output\n",
        "```\n",
        "This is an example of a positive tweet: \n",
        " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
        " \n",
        "This is an example of the processes version: \n",
        " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ9obDRn7SI4"
      },
      "source": [
        "# Part 1: Logistic regression \n",
        "\n",
        "\n",
        "### Part 1.1: Sigmoid\n",
        "You will learn to use logistic regression for text classification. \n",
        "* The sigmoid function is defined as: \n",
        "\n",
        "$$ h(z) = \\frac{1}{1+\\exp^{-z}} \\tag{1}$$\n",
        "\n",
        "It maps the input 'z' to a value that ranges between 0 and 1, and so it can be treated as a probability. \n",
        "\n",
        "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='../tmp2/sigmoid_plot.jpg' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:300px;height:200px;\" /> Figure 1 </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGzenJbW7SI4"
      },
      "source": [
        "#### Instructions: Implement the sigmoid function\n",
        "* You will want this function to work if z is a scalar as well as if it is an array."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Tg5qDeq7SI5"
      },
      "source": [
        "<details>    \n",
        "<summary>\n",
        "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
        "</summary>\n",
        "<p>\n",
        "<ul>\n",
        "    <li><a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html\" > numpy.exp </a> </li>\n",
        "\n",
        "</ul>\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "TVwNkbG_7SI5"
      },
      "outputs": [],
      "source": [
        "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "def sigmoid(z): \n",
        "    '''\n",
        "    Input:\n",
        "        z: is the input (can be a scalar or an array)\n",
        "    Output:\n",
        "        h: the sigmoid of z\n",
        "    '''\n",
        "    \n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "    # calculate the sigmoid of z\n",
        "    h = 1 / (1 + np.exp(-z))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEJmPVYb7SI6",
        "outputId": "6b80de3d-d21a-41e2-a20d-e569770cd3d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUCCESS!\n",
            "CORRECT!\n"
          ]
        }
      ],
      "source": [
        "# Testing your function \n",
        "if (sigmoid(0) == 0.5):\n",
        "    print('SUCCESS!')\n",
        "else:\n",
        "    print('Oops!')\n",
        "\n",
        "if (sigmoid(4.92) == 0.9927537604041685):\n",
        "    print('CORRECT!')\n",
        "else:\n",
        "    print('Oops again!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zb2eouY97SI7"
      },
      "source": [
        "### Logistic regression: regression and a sigmoid\n",
        "\n",
        "Logistic regression takes a regular linear regression, and applies a sigmoid to the output of the linear regression.\n",
        "\n",
        "Regression:\n",
        "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
        "Note that the $\\theta$ values are \"weights\". If you took the Deep Learning Specialization, we referred to the weights with the `w` vector.  In this course, we're using a different variable $\\theta$ to refer to the weights.\n",
        "\n",
        "Logistic regression\n",
        "$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$\n",
        "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
        "We will refer to 'z' as the 'logits'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGw1fy_A7SI7"
      },
      "source": [
        "### Part 1.2 Cost function and Gradient\n",
        "\n",
        "The cost function used for logistic regression is the average of the log loss across all training examples:\n",
        "\n",
        "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))\\tag{5} $$\n",
        "* $m$ is the number of training examples\n",
        "* $y^{(i)}$ is the actual label of the i-th training example.\n",
        "* $h(z(\\theta)^{(i)})$ is the model's prediction for the i-th training example.\n",
        "\n",
        "The loss function for a single training example is\n",
        "$$ Loss = -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)$$\n",
        "\n",
        "* All the $h$ values are between 0 and 1, so the logs will be negative. That is the reason for the factor of -1 applied to the sum of the two loss terms.\n",
        "* Note that when the model predicts 1 ($h(z(\\theta)) = 1$) and the label $y$ is also 1, the loss for that training example is 0. \n",
        "* Similarly, when the model predicts 0 ($h(z(\\theta)) = 0$) and the actual label is also 0, the loss for that training example is 0. \n",
        "* However, when the model prediction is close to 1 ($h(z(\\theta)) = 0.9999$) and the label is 0, the second term of the log loss becomes a large negative number, which is then multiplied by the overall factor of -1 to convert it to a positive loss value. $-1 \\times (1 - 0) \\times log(1 - 0.9999) \\approx 9.2$ The closer the model prediction gets to 1, the larger the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfiJMZb77SI7",
        "outputId": "07b5d813-09c3-44f2-e028-0905ed89670c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.210340371976294"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# verify that when the model predicts close to 1, but the actual label is 0, the loss is a large positive value\n",
        "-1 * (1 - 0) * np.log(1 - 0.9999) # loss is about 9.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sePPDRFo7SI8"
      },
      "source": [
        "* Likewise, if the model predicts close to 0 ($h(z) = 0.0001$) but the actual label is 1, the first term in the loss function becomes a large number: $-1 \\times log(0.0001) \\approx 9.2$.  The closer the prediction is to zero, the larger the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QB5CUSF07SI8",
        "outputId": "394c01c8-b830-4b90-a5ab-ad4b0fd42999"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.210340371976182"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# verify that when the model predicts close to 0 but the actual label is 1, the loss is a large positive value\n",
        "-1 * np.log(0.0001) # loss is about 9.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlYQ9OOf7SI9"
      },
      "source": [
        "#### Update the weights\n",
        "\n",
        "To update your weight vector $\\theta$, you will apply gradient descent to iteratively improve your model's predictions.  \n",
        "The gradient of the cost function $J$ with respect to one of the weights $\\theta_j$ is:\n",
        "\n",
        "$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x_j \\tag{5}$$\n",
        "* 'i' is the index across all 'm' training examples.\n",
        "* 'j' is the index of the weight $\\theta_j$, so $x_j$ is the feature associated with weight $\\theta_j$\n",
        "\n",
        "* To update the weight $\\theta_j$, we adjust it by subtracting a fraction of the gradient determined by $\\alpha$:\n",
        "$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$\n",
        "* The learning rate $\\alpha$ is a value that we choose to control how big a single update will be.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmuag6_z7SI9"
      },
      "source": [
        "## Instructions: Implement gradient descent function\n",
        "* The number of iterations `num_iters` is the number of times that you'll use the entire training set.\n",
        "* For each iteration, you'll calculate the cost function using all training examples (there are `m` training examples), and for all features.\n",
        "* Instead of updating a single weight $\\theta_i$ at a time, we can update all the weights in the column vector:  \n",
        "$$\\mathbf{\\theta} = \\begin{pmatrix}\n",
        "\\theta_0\n",
        "\\\\\n",
        "\\theta_1\n",
        "\\\\ \n",
        "\\theta_2 \n",
        "\\\\ \n",
        "\\vdots\n",
        "\\\\ \n",
        "\\theta_n\n",
        "\\end{pmatrix}$$\n",
        "* $\\mathbf{\\theta}$ has dimensions (n+1, 1), where 'n' is the number of features, and there is one more element for the bias term $\\theta_0$ (note that the corresponding feature value $\\mathbf{x_0}$ is 1).\n",
        "* The 'logits', 'z', are calculated by multiplying the feature matrix 'x' with the weight vector 'theta'.  $z = \\mathbf{x}\\mathbf{\\theta}$\n",
        "    * $\\mathbf{x}$ has dimensions (m, n+1) \n",
        "    * $\\mathbf{\\theta}$: has dimensions (n+1, 1)\n",
        "    * $\\mathbf{z}$: has dimensions (m, 1)\n",
        "* The prediction 'h', is calculated by applying the sigmoid to each element in 'z': $h(z) = sigmoid(z)$, and has dimensions (m,1).\n",
        "* The cost function $J$ is calculated by taking the dot product of the vectors 'y' and 'log(h)'.  Since both 'y' and 'h' are column vectors (m,1), transpose the vector to the left, so that matrix multiplication of a row vector with column vector performs the dot product.\n",
        "$$J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)$$\n",
        "* The update of theta is also vectorized.  Because the dimensions of $\\mathbf{x}$ are (m, n+1), and both $\\mathbf{h}$ and $\\mathbf{y}$ are (m, 1), we need to transpose the $\\mathbf{x}$ and place it on the left in order to perform matrix multiplication, which then yields the (n+1, 1) answer we need:\n",
        "$$\\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5DBy32K7SI-"
      },
      "source": [
        "<details>    \n",
        "<summary>\n",
        "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
        "</summary>\n",
        "<p>\n",
        "<ul>\n",
        "    <li>use np.dot for matrix multiplication.</li>\n",
        "    <li>To ensure that the fraction -1/m is a decimal value, cast either the numerator or denominator (or both), like `float(1)`, or write `1.` for the float version of 1. </li>\n",
        "</ul>\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "jRtWtK5U7SI-"
      },
      "outputs": [],
      "source": [
        "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "    '''\n",
        "    Input:\n",
        "        x: matrix of features which is (m,n+1)\n",
        "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
        "        theta: weight vector of dimension (n+1,1)\n",
        "        alpha: learning rate\n",
        "        num_iters: number of iterations you want to train your model for\n",
        "    Output:\n",
        "        J: the final cost\n",
        "        theta: your final weight vector\n",
        "    Hint: you might want to print the cost to make sure that it is going down.\n",
        "    '''\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "    # get 'm', the number of rows in matrix x\n",
        "    m = x.shape[0]\n",
        "    \n",
        "    for i in range(0, num_iters):\n",
        "        \n",
        "        # get z, the dot product of x and theta\n",
        "        z = np.dot(x,theta)\n",
        "        \n",
        "        # get the sigmoid of h\n",
        "        h = sigmoid(z)\n",
        "        \n",
        "        # calculate the cost function\n",
        "        # note that we can use also np.array.transpose() instead of np.array.T\n",
        "        # np.array.T just makes code a little more readable :)\n",
        "        \n",
        "        J = -1./m * (np.dot(y.T, np.log(h)) + np.dot((1-y).T, np.log(1-h)))                                                  \n",
        "\n",
        "        # update the weights theta\n",
        "        theta -= alpha/m * (np.dot(x.T,(h-y)))\n",
        "        \n",
        "    ### END CODE HERE ###\n",
        "    J = float(J)\n",
        "    return J, theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0K8AqQ7I7SI_",
        "outputId": "f0af5b89-f588-4c61-e273-4c9cb691d847"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cost after training is 0.67094970.\n",
            "The resulting vector of weights is [4.1e-07, 0.00035658, 7.309e-05]\n"
          ]
        }
      ],
      "source": [
        "# Check the function\n",
        "# Construct a synthetic test case using numpy PRNG functions\n",
        "np.random.seed(1)\n",
        "# X input is 10 x 3 with ones for the bias terms\n",
        "tmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)\n",
        "# Y Labels are 10 x 1\n",
        "tmp_Y = (np.random.rand(10, 1) > 0.35).astype(float)\n",
        "\n",
        "# Apply gradient descent\n",
        "tmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 700)\n",
        "print(f\"The cost after training is {tmp_J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(tmp_theta)]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbnRUkaZ7SJA"
      },
      "source": [
        "#### Expected output\n",
        "```\n",
        "The cost after training is 0.67094970.\n",
        "The resulting vector of weights is [4.1e-07, 0.00035658, 7.309e-05]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqpkD3dp7SJA"
      },
      "source": [
        "## Part 2: Extracting the features\n",
        "\n",
        "* Given a list of tweets, extract the features and store them in a matrix. You will extract two features.\n",
        "    * The first feature is the number of positive words in a tweet.\n",
        "    * The second feature is the number of negative words in a tweet. \n",
        "* Then train your logistic regression classifier on these features.\n",
        "* Test the classifier on a validation set. \n",
        "\n",
        "### Instructions: Implement the extract_features function. \n",
        "* This function takes in a single tweet.\n",
        "* Process the tweet using the imported `process_tweet()` function and save the list of tweet words.\n",
        "* Loop through each word in the list of processed words\n",
        "    * For each word, check the `freqs` dictionary for the count when that word has a positive '1' label. (Check for the key (word, 1.0)\n",
        "    * Do the same for the count for when the word is associated with the negative label '0'. (Check for the key (word, 0.0).)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU9ZOuRC7SJB"
      },
      "source": [
        "<details>    \n",
        "<summary>\n",
        "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
        "</summary>\n",
        "<p>\n",
        "<ul>\n",
        "    <li>Make sure you handle cases when the (word, label) key is not found in the dictionary. </li>\n",
        "    <li> Search the web for hints about using the `.get()` method of a Python dictionary.  Here is an <a href=\"https://www.programiz.com/python-programming/methods/dictionary/get\" > example </a> </li>\n",
        "</ul>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**dictionary.get(keyname, value)**\n",
        "\n",
        "**keyname**\t: Required. The keyname of the item you want to return the value from\n",
        "\n",
        "**value** :\tOptional. A value to return if the specified key does not exist.\n",
        "Default value None"
      ],
      "metadata": {
        "id": "4gkBZ7_7AJJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(freqs.get(('asdgh', 1.0),6.6)) # there is no 'asdgh' key \n",
        "print(freqs.get(('show', 1.0),6.6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nRhlp2WA6Kn",
        "outputId": "32087d8a-93c8-474d-8ceb-63312edf5578"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.6\n",
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "CqddRbPD7SJB"
      },
      "outputs": [],
      "source": [
        "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "def extract_features(tweet, freqs):\n",
        "    '''\n",
        "    Input: \n",
        "        tweet: a list of words for one tweet\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "    Output: \n",
        "        x: a feature vector of dimension (1,3)\n",
        "    '''\n",
        "    # process_tweet tokenizes, stems, and removes stopwords\n",
        "    word_l = process_tweet(tweet)\n",
        "    \n",
        "    # 3 elements in the form of a 1 x 3 vector\n",
        "    x = np.zeros((1,3))\n",
        "    \n",
        "    #bias term is set to 1\n",
        "    x[0,0] = 1\n",
        "    \n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "    \n",
        "    # loop through each word in the list of words\n",
        "    for word in word_l:\n",
        "        \n",
        "        # increment the word count for the positive label 1\n",
        "        x[0,1] = freqs.get((word,1.),0)\n",
        "        \n",
        "        # increment the word count for the negative label 0\n",
        "        x[0,2] = freqs.get((word,0.),0)\n",
        "        \n",
        "    ### END CODE HERE ###\n",
        "    assert(x.shape == (1, 3))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDbYt3si7SJB",
        "outputId": "5fc8db68-c89d-4b9a-f921-5c035fdfa4c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.000e+00 2.847e+03 2.000e+00]]\n"
          ]
        }
      ],
      "source": [
        "# Check your function\n",
        "\n",
        "# test 1\n",
        "# test on training data\n",
        "tmp1 = extract_features(train_x[0], freqs)\n",
        "print(tmp1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMuvTs9q7SJC"
      },
      "source": [
        "#### Expected output\n",
        "```\n",
        "[[1.00e+00 3.02e+03 6.10e+01]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOkicCuD7SJC",
        "outputId": "d2828ef0-c0aa-4083-ce5f-bae302ea40a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "# test 2:\n",
        "# check for when the words are not in the freqs dictionary\n",
        "tmp2 = extract_features('blorb bleeeeb bloooob', freqs)\n",
        "print(tmp2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P5T4z867SJD"
      },
      "source": [
        "#### Expected output\n",
        "```\n",
        "[[1. 0. 0.]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2_yWb7N7SJD"
      },
      "source": [
        "## Part 3: Training Your Model\n",
        "\n",
        "To train the model:\n",
        "* Stack the features for all training examples into a matrix `X`. \n",
        "* Call `gradientDescent`, which you've implemented above.\n",
        "\n",
        "This section is given to you.  Please read it for understanding and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywDX10l67SJD",
        "outputId": "d6f1b710-bb72-4874-8957-87884fdcb6ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cost after training is 0.31770499.\n",
            "The resulting vector of weights is [1.8e-07, 0.00074006, -0.00077059]\n"
          ]
        }
      ],
      "source": [
        "# collect the features 'x' and stack them into a matrix 'X'\n",
        "X = np.zeros((len(train_x), 3))\n",
        "for i in range(len(train_x)):\n",
        "    X[i, :]= extract_features(train_x[i], freqs)\n",
        "\n",
        "# training labels corresponding to X\n",
        "Y = train_y\n",
        "\n",
        "# Apply gradient descent\n",
        "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 4000)\n",
        "print(f\"The cost after training is {J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvtHHJEN7SJE"
      },
      "source": [
        "**Expected Output**: \n",
        "\n",
        "```\n",
        "The cost after training is 0.24216529.\n",
        "The resulting vector of weights is [7e-08, 0.0005239, -0.00055517]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR43kmHH7SJE"
      },
      "source": [
        "# Part 4: Test your logistic regression\n",
        "\n",
        "It is time for you to test your logistic regression function on some new input that your model has not seen before. \n",
        "\n",
        "#### Instructions: Write `predict_tweet`\n",
        "Predict whether a tweet is positive or negative.\n",
        "\n",
        "* Given a tweet, process it, then extract the features.\n",
        "* Apply the model's learned weights on the features to get the logits.\n",
        "* Apply the sigmoid to the logits to get the prediction (a value between 0 and 1).\n",
        "\n",
        "$$y_{pred} = sigmoid(\\mathbf{x} \\cdot \\theta)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "OfFGSLE77SJF"
      },
      "outputs": [],
      "source": [
        "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "def predict_tweet(tweet, freqs, theta):\n",
        "    '''\n",
        "    Input: \n",
        "        tweet: a string\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "        theta: (3,1) vector of weights\n",
        "    Output: \n",
        "        y_pred: the probability of a tweet being positive or negative\n",
        "    '''\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "    \n",
        "    # extract the features of the tweet and store it into x\n",
        "    x = extract_features(tweet,freqs)\n",
        "    \n",
        "    # make the prediction using x and theta\n",
        "    y_pred = sigmoid(np.dot(x,theta)) #x.shape=(1,3)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-tEwbGx7SJF",
        "outputId": "a7b344b2-6cc6-4162-d917-f8f54c7a2f4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am happy -> 0.517189\n",
            "I am bad -> 0.494023\n",
            "this movie should have been great. -> 0.514305\n",
            "great -> 0.514305\n",
            "great great -> 0.514305\n",
            "great great great -> 0.514305\n",
            "great great great great -> 0.514305\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to test your function\n",
        "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
        "    print( '%s -> %f' % (tweet, predict_tweet(tweet, freqs, theta)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K4r9YOG7SJG"
      },
      "source": [
        "**Expected Output**: \n",
        "```\n",
        "I am happy -> 0.518580\n",
        "I am bad -> 0.494339\n",
        "this movie should have been great. -> 0.515331\n",
        "great -> 0.515464\n",
        "great great -> 0.530898\n",
        "great great great -> 0.546273\n",
        "great great great great -> 0.561561\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfCwHBrA7SJG",
        "outputId": "1513dc7e-3f4a-4759-82c0-758d52e452a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.80164547]])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "# Feel free to check the sentiment of your own tweet below\n",
        "my_tweet = 'I am learning :)'\n",
        "predict_tweet(my_tweet, freqs, theta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdIkzqN57SJG"
      },
      "source": [
        "## Check performance using the test set\n",
        "After training your model using the training set above, check how your model might perform on real, unseen data, by testing it against the test set.\n",
        "\n",
        "#### Instructions: Implement `test_logistic_regression` \n",
        "* Given the test data and the weights of your trained model, calculate the accuracy of your logistic regression model. \n",
        "* Use your `predict_tweet()` function to make predictions on each tweet in the test set.\n",
        "* If the prediction is > 0.5, set the model's classification `y_hat` to 1, otherwise set the model's classification `y_hat` to 0.\n",
        "* A prediction is accurate when `y_hat` equals `test_y`.  Sum up all the instances when they are equal and divide by `m`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX2Okjld7SJH"
      },
      "source": [
        "<details>    \n",
        "<summary>\n",
        "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
        "</summary>\n",
        "<p>\n",
        "<ul>\n",
        "    <li>Use np.asarray() to convert a list to a numpy array</li>\n",
        "    <li>Use np.squeeze() to make an (m,1) dimensional array into an (m,) array </li>\n",
        "</ul>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "a5oEWXw27SJH"
      },
      "outputs": [],
      "source": [
        "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
        "    \"\"\"\n",
        "    Input: \n",
        "        test_x: a list of tweets\n",
        "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
        "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "        theta: weight vector of dimension (3, 1)\n",
        "    Output: \n",
        "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "    \n",
        "    # the list for storing predictions\n",
        "    y_hat = []\n",
        "    \n",
        "    for tweet in test_x:\n",
        "        # get the label prediction for the tweet\n",
        "        y_pred = predict_tweet(tweet,freqs,theta)\n",
        "        \n",
        "        if y_pred > 0.5:\n",
        "            # append 1.0 to the list\n",
        "            y_hat.append(1)\n",
        "        else:\n",
        "            # append 0 to the list\n",
        "            y_hat.append(0)\n",
        "\n",
        "    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
        "    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
        "    \n",
        "    accuracy = (y_hat==np.squeeze(test_y)).sum()/len(test_x)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uS9Z_J3q7SJH",
        "outputId": "45a01e05-bd2a-4ab1-ea3c-926eb53dfa8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression model's accuracy = 0.8860\n"
          ]
        }
      ],
      "source": [
        "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
        "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk7aopJB7SJI"
      },
      "source": [
        "#### Expected Output: \n",
        "```0.9950```  \n",
        "Pretty good!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__nSzITA7SJI"
      },
      "source": [
        "# Part 5: Error Analysis\n",
        "\n",
        "In this part you will see some tweets that your model misclassified. Why do you think the misclassifications happened? Specifically what kind of tweets does your model misclassify?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H_ylqnc7SJI",
        "outputId": "c2f43be9-51b7-4b5b-8e6f-7aad637c1634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Predicted Tweet\n",
            "THE TWEET IS: Good morning all :-)\n",
            "\n",
            "It's Friday!!!!!! 󾰀\n",
            "\n",
            "What are your plans for the day? I am currently playing shops with my... http://t.co/qoKquDWcb5\n",
            "THE PROCESSED TWEET IS: ['good', 'morn', ':-)', 'friday', '\\U000fec00', 'plan', 'day', 'current', 'play', 'shop', '...']\n",
            "1\t0.48767431\tb'good morn :-) friday  plan day current play shop ...'\n",
            "THE TWEET IS: @SasaRichardson @Stefbystef_ @Frgt10_Anthem HAHAHAHAHAHAHAHAHAHAHAHAHAHA I AM DYING :)) I AM LITERALLY IN FRONT OF IT AND I WAS LIKE\n",
            "THE PROCESSED TWEET IS: ['hahahahahahahahahahahahahaha', 'die', ':)', 'liter', 'front', 'like']\n",
            "1\t0.49953599\tb'hahahahahahahahahahahahahaha die :) liter front like'\n",
            "THE TWEET IS: @Ms_chic I know :), you're hot always\n",
            "THE PROCESSED TWEET IS: ['know', ':)', 'hot', 'alway']\n",
            "1\t0.49963368\tb'know :) hot alway'\n",
            "THE TWEET IS: @batesm0t3l Hi there, I've spoken with the store who have advised they do have a PayPoint and that card can be topped up :) Thanks, Beth\n",
            "THE PROCESSED TWEET IS: ['hi', \"i'v\", 'spoken', 'store', 'advis', 'paypoint', 'card', 'top', ':)', 'thank', 'beth']\n",
            "1\t0.49979977\tb\"hi i'v spoken store advis paypoint card top :) thank beth\"\n",
            "THE TWEET IS: @Madison420Ivy  \"We\" ??? you wish :D:D:D:D:D:D:D kidding\n",
            "THE PROCESSED TWEET IS: ['wish', ':d', ':d', ':d', ':d', ':d', ':d', ':d', 'kid']\n",
            "1\t0.49932288\tb'wish :d :d :d :d :d :d :d kid'\n",
            "THE TWEET IS: @Svg_Brad Congrads !!! Better update your About page on the accomplishment !!! :-) Keep it up...\n",
            "THE PROCESSED TWEET IS: ['congrad', 'better', 'updat', 'page', 'accomplish', ':-)', 'keep', '...']\n",
            "1\t0.48767431\tb'congrad better updat page accomplish :-) keep ...'\n",
            "THE TWEET IS: @kezhoskyn We got a call in the end and the pigeon is now home safe :) Definitely worth contacting them.\n",
            "THE PROCESSED TWEET IS: ['got', 'call', 'end', 'pigeon', 'home', 'safe', ':)', 'definit', 'worth', 'contact']\n",
            "1\t0.49939157\tb'got call end pigeon home safe :) definit worth contact'\n",
            "THE TWEET IS: goodnight guys :-) \n",
            "remember tomorrow is a brand new day, a fresh start and another chance\n",
            "THE PROCESSED TWEET IS: ['goodnight', 'guy', ':-)', 'rememb', 'tomorrow', 'brand', 'new', 'day', 'fresh', 'start', 'anoth', 'chanc']\n",
            "1\t0.49785803\tb'goodnight guy :-) rememb tomorrow brand new day fresh start anoth chanc'\n",
            "THE TWEET IS: Bom-dia :) APOD: Ultraviolet Rings of M31 (2015 Jul 24) - \n",
            "http://t.co/f5xThyIN8u -\n",
            "THE PROCESSED TWEET IS: ['bom-dia', ':)', 'apod', 'ultraviolet', 'ring', 'm31', '2015', 'jul', '24']\n",
            "1\t0.49882126\tb'bom-dia :) apod ultraviolet ring m31 2015 jul 24'\n",
            "THE TWEET IS: Gud afterznoon and jumma mubarak tweeeps :-)\n",
            "Plz remember us in ur prayers\n",
            "THE PROCESSED TWEET IS: ['gud', 'afterznoon', 'jumma', 'mubarak', 'tweeep', ':-)', 'plz', 'rememb', 'us', 'ur', 'prayer']\n",
            "1\t0.49998478\tb'gud afterznoon jumma mubarak tweeep :-) plz rememb us ur prayer'\n",
            "THE TWEET IS: next year for sure though :) there's only 2 girls that piss me off\n",
            "THE PROCESSED TWEET IS: ['next', 'year', 'sure', 'though', ':)', \"there'\", '2', 'girl', 'piss']\n",
            "1\t0.49998478\tb\"next year sure though :) there' 2 girl piss\"\n",
            "THE TWEET IS: Towering over the ladies. :) #tb #eid https://t.co/F7oS1h3Alz\n",
            "THE PROCESSED TWEET IS: ['tower', 'ladi', ':)', 'tb', 'eid']\n",
            "1\t0.49941447\tb'tower ladi :) tb eid'\n",
            "THE TWEET IS: Whatsapp with roommate\n",
            "\n",
            "\"Do you want anything from Paris?\"\n",
            "\"A French man :)\"\n",
            "\n",
            "LOL sure I'll just head to the park and grab one.\n",
            "THE PROCESSED TWEET IS: ['whatsapp', 'roommat', 'want', 'anyth', 'pari', 'french', 'man', ':)', 'lol', 'sure', \"i'll\", 'head', 'park', 'grab', 'one']\n",
            "1\t0.49411189\tb\"whatsapp roommat want anyth pari french man :) lol sure i'll head park grab one\"\n",
            "THE TWEET IS: @AndyCarnochan @WickermanFest Oops...that's called a #FridayFauxPas :) I'll get that changed right now! Thank you, G\n",
            "THE PROCESSED TWEET IS: ['oop', '...', \"that'\", 'call', 'fridayfauxpa', ':)', \"i'll\", 'get', 'chang', 'right', 'thank', 'g']\n",
            "1\t0.49994662\tb\"oop ... that' call fridayfauxpa :) i'll get chang right thank g\"\n",
            "THE TWEET IS: @balboa_lesly Thank you for the screenshots po. Let me take care of this spammer for you. :) Before we proceed, c... https://t.co/x4ObmZFYUB\n",
            "THE PROCESSED TWEET IS: ['thank', 'screenshot', 'po', 'let', 'take', 'care', 'spammer', ':)', 'proceed', 'c', '...']\n",
            "1\t0.48767431\tb'thank screenshot po let take care spammer :) proceed c ...'\n",
            "THE TWEET IS: saw a comment on how midares gender is unknown and like\n",
            ":) yeah you just keep saying that\n",
            "THE PROCESSED TWEET IS: ['saw', 'comment', 'midar', 'gender', 'unknown', 'like', ':)', 'yeah', 'keep', 'say']\n",
            "1\t0.49890125\tb'saw comment midar gender unknown like :) yeah keep say'\n",
            "THE TWEET IS: Thank you @AngelMsvanityIV.... loved the cakes :) bear hug at the end\n",
            "THE PROCESSED TWEET IS: ['thank', '...', 'love', 'cake', ':)', 'bear', 'hug', 'end']\n",
            "1\t0.49547005\tb'thank ... love cake :) bear hug end'\n",
            "THE TWEET IS: Amazing pics today on http://t.co/dcifNBjTXA @kellyhallmodel  :D http://t.co/Ja2vPugANE\n",
            "THE PROCESSED TWEET IS: ['amaz', 'pic', 'today']\n",
            "1\t0.49895835\tb'amaz pic today'\n",
            "THE TWEET IS: @briones198 always makes things better :)❤️\n",
            "THE PROCESSED TWEET IS: ['alway', 'make', 'thing', 'better', ':)', '❤', '️']\n",
            "1\t0.49972344\tb'alway make thing better :)  '\n",
            "THE TWEET IS: @Stuarty2112 :-)  Cheers Pete. Will have a listen later. ☺☺\n",
            "THE PROCESSED TWEET IS: [':-)', 'cheer', 'pete', 'listen', 'later', '☺', '☺']\n",
            "1\t0.49998478\tb':-) cheer pete listen later  '\n",
            "THE TWEET IS: @Cat_grl @berlindisaster BOOM. It's a date! :D \"z\n",
            "THE PROCESSED TWEET IS: ['boom', 'date', ':d', 'z']\n",
            "1\t0.49942210\tb'boom date :d z'\n",
            "THE TWEET IS: Goooood Mooorning :)\n",
            "I want to sleep again...\n",
            "THE PROCESSED TWEET IS: ['goood', 'mooorn', ':)', 'want', 'sleep', '...']\n",
            "1\t0.48767431\tb'goood mooorn :) want sleep ...'\n",
            "THE TWEET IS: Crazy girlfriends be like :-)(-: Jesus Christ. http://t.co/RTWjc7e1lM\n",
            "THE PROCESSED TWEET IS: ['crazi', 'girlfriend', 'like', ':-)', '(-:', 'jesu', 'christ']\n",
            "1\t0.49980740\tb'crazi girlfriend like :-) (-: jesu christ'\n",
            "THE TWEET IS: nd its going to expire next year. dat is mtn 6gb for u :D , You really like it?\n",
            "THE PROCESSED TWEET IS: ['nd', 'go', 'expir', 'next', 'year', 'dat', 'mtn', '6gb', 'u', ':d', 'realli', 'like']\n",
            "1\t0.49953599\tb'nd go expir next year dat mtn 6gb u :d realli like'\n",
            "THE TWEET IS: @melaniemorris Hi Melanie :-) We do in fact, they're really used in cases where normal WiFi could be an issue as you mentioned. You can ....\n",
            "THE PROCESSED TWEET IS: ['hi', 'melani', ':-)', 'fact', \"they'r\", 'realli', 'use', 'case', 'normal', 'wifi', 'could', 'issu', 'mention', '...']\n",
            "1\t0.48767431\tb\"hi melani :-) fact they'r realli use case normal wifi could issu mention ...\"\n",
            "THE TWEET IS: Only my bad would remind me to exercise at 1:12 in the am :-). \n",
            " I miss her. She needs to come back. Now.\n",
            "THE PROCESSED TWEET IS: ['bad', 'would', 'remind', 'exercis', '1:12', ':-)', 'miss', 'need', 'come', 'back']\n",
            "1\t0.49481187\tb'bad would remind exercis 1:12 :-) miss need come back'\n",
            "THE TWEET IS: @Junaidogic kyunke Aitchison hai :p look at the responses to my original tweet, you'll know why my brother does this\n",
            "THE PROCESSED TWEET IS: ['kyunk', 'aitchison', 'hai', ':p', 'look', 'respons', 'origin', 'tweet', 'know', 'brother']\n",
            "1\t0.49882126\tb'kyunk aitchison hai :p look respons origin tweet know brother'\n",
            "THE TWEET IS: @Bixbersboca good morning to you :D lol its really fuckin dark and it's gonna rain so hard in a couple of minutes\n",
            "THE PROCESSED TWEET IS: ['good', 'morn', ':d', 'lol', 'realli', 'fuckin', 'dark', 'gonna', 'rain', 'hard', 'coupl', 'minut']\n",
            "1\t0.49976923\tb'good morn :d lol realli fuckin dark gonna rain hard coupl minut'\n",
            "THE TWEET IS: NeoBytes :) Universal Pictures seeds pirated movie copy, files takedown notice against itself http://t.co/4ESrPPRXCA\n",
            "THE PROCESSED TWEET IS: ['neobyt', ':)', 'univers', 'pictur', 'seed', 'pirat', 'movi', 'copi', 'file', 'takedown', 'notic']\n",
            "1\t0.49839781\tb'neobyt :) univers pictur seed pirat movi copi file takedown notic'\n",
            "THE TWEET IS: @bryantduncan98 of course man :) Nofx is my shit\n",
            "THE PROCESSED TWEET IS: ['cours', 'man', ':)', 'nofx', 'shit']\n",
            "1\t0.49702640\tb'cours man :) nofx shit'\n",
            "THE TWEET IS: @katelynlander GZ :D what did you named it =^.^=\n",
            "THE PROCESSED TWEET IS: ['gz', ':d', 'name']\n",
            "1\t0.49971581\tb'gz :d name'\n",
            "THE TWEET IS: @RK1610IsMe Hai Rajeev..hope u r hvng a funfilled friday.It ws realy amazing to c diff side of Kabir.Fell in luv with u again :) A smile pls\n",
            "THE PROCESSED TWEET IS: ['hai', 'rajeev', '..', 'hope', 'u', 'r', 'hvng', 'funfil', 'friday.it', 'ws', 'reali', 'amaz', 'c', 'diff', 'side', 'kabir.fel', 'luv', 'u', ':)', 'smile', 'pl']\n",
            "1\t0.49648665\tb'hai rajeev .. hope u r hvng funfil friday.it ws reali amaz c diff side kabir.fel luv u :) smile pl'\n",
            "THE TWEET IS: @ConnellJess you wanna be a loser like me ? :) ♥\n",
            "THE PROCESSED TWEET IS: ['wanna', 'loser', 'like', ':)', '♥']\n",
            "1\t0.49855229\tb'wanna loser like :) '\n",
            "THE TWEET IS: YAS. \n",
            "Thanks @cm_valladolid for these :) 😊 https://t.co/4rNoGpudcX\n",
            "THE PROCESSED TWEET IS: ['ya', 'thank', ':)', '😊']\n",
            "1\t0.49993135\tb'ya thank :) '\n",
            "THE TWEET IS: niall followed a fan :) and i'm still here without his follow 👍\n",
            "THE PROCESSED TWEET IS: ['niall', 'follow', 'fan', ':)', \"i'm\", 'still', 'without', 'follow', '👍']\n",
            "1\t0.49980740\tb\"niall follow fan :) i'm still without follow \"\n",
            "THE TWEET IS: @mistytewest Happy to save the day :) Hope you enjoyed your meal Sam!\n",
            "THE PROCESSED TWEET IS: ['happi', 'save', 'day', ':)', 'hope', 'enjoy', 'meal', 'sam']\n",
            "1\t0.49959948\tb'happi save day :) hope enjoy meal sam'\n",
            "THE TWEET IS: 5G Liker :)\n",
            ".\n",
            "like Fast :*\n",
            "THE PROCESSED TWEET IS: ['5g', 'liker', ':)', 'like', 'fast']\n",
            "1\t0.49921419\tb'5g liker :) like fast'\n",
            "THE TWEET IS: @vapemestoopid Ok,the first time we chat,and then i made such a joke lol .I believe you wont forget me,will u ? :) my name is @DannaQiu\n",
            "THE PROCESSED TWEET IS: ['ok', 'first', 'time', 'chat', 'made', 'joke', 'lol', 'believ', 'wont', 'forget', 'u', ':)', 'name']\n",
            "1\t0.49971581\tb'ok first time chat made joke lol believ wont forget u :) name'\n",
            "THE TWEET IS: @UNAPCTT Thanks for adding us to your list! :) Make sure to keep in touch for more news of our light bulbs made from corn waste.\n",
            "THE PROCESSED TWEET IS: ['thank', 'ad', 'us', 'list', ':)', 'make', 'sure', 'keep', 'touch', 'news', 'light', 'bulb', 'made', 'corn', 'wast']\n",
            "1\t0.49922182\tb'thank ad us list :) make sure keep touch news light bulb made corn wast'\n",
            "THE TWEET IS: E.L.F: \"Thank you for being a singer :)\" | Kyuhyun: \"Thank you for being E.L.F\"\n",
            "THE PROCESSED TWEET IS: ['e', 'l', 'f', 'thank', 'singer', ':)', 'kyuhyun', 'thank', 'e', 'l', 'f']\n",
            "1\t0.49922945\tb'e l f thank singer :) kyuhyun thank e l f'\n",
            "THE TWEET IS: My word, @anaturalwedding &amp; @TheStoveRoom - what big sites you've got! Changes a-foot :) #website #development #revamp\n",
            "THE PROCESSED TWEET IS: ['word', 'big', 'site', 'got', 'chang', 'a-foot', ':)', 'websit', 'develop', 'revamp']\n",
            "1\t0.49980740\tb'word big site got chang a-foot :) websit develop revamp'\n",
            "THE TWEET IS: Add me on the snapchat yall :) give me your @ names\n",
            "THE PROCESSED TWEET IS: ['add', 'snapchat', 'yall', ':)', 'give', 'name']\n",
            "1\t0.49971581\tb'add snapchat yall :) give name'\n",
            "THE TWEET IS: @6townsradio am tuned back in. With hubby :) can u play him queen pls\n",
            "THE PROCESSED TWEET IS: ['tune', 'back', 'hubbi', ':)', 'u', 'play', 'queen', 'pl']\n",
            "1\t0.49648665\tb'tune back hubbi :) u play queen pl'\n",
            "THE TWEET IS: First leg of the @magictrikband tour is going well! :D #music #band #rock #magictrik #tour https://t.co/CQHqWfa7ft\n",
            "THE PROCESSED TWEET IS: ['first', 'leg', 'tour', 'go', 'well', ':d', 'music', 'band', 'rock', 'magictrik', 'tour']\n",
            "1\t0.49939157\tb'first leg tour go well :d music band rock magictrik tour'\n",
            "THE TWEET IS: its leo season :-) ♌️\n",
            "THE PROCESSED TWEET IS: ['leo', 'season', ':-)', '♌', '️']\n",
            "1\t0.49972344\tb'leo season :-)  '\n",
            "THE TWEET IS: @pussy_lilac No problem :) it's a pleasure ❤️\n",
            "THE PROCESSED TWEET IS: ['problem', ':)', 'pleasur', '❤', '️']\n",
            "1\t0.49972344\tb'problem :) pleasur  '\n",
            "THE TWEET IS: @lazariWilliams Hey, You like FNaF? Check out our Youtube Channel! https://t.co/sc9kDhaviX :) via http://t.co/J3sxzzg7cU\n",
            "THE PROCESSED TWEET IS: ['hey', 'like', 'fnaf', 'check', 'youtub', 'channel']\n",
            "1\t0.49959948\tb'hey like fnaf check youtub channel'\n",
            "THE TWEET IS: @Mathpro314 Hey, wanna check out our YTB Channel? We post Gameplays &amp; Tutorials! https://t.co/sc9kDhaviX :) via http://t.co/J3sxzzg7cU\n",
            "THE PROCESSED TWEET IS: ['hey', 'wanna', 'check', 'ytb', 'channel', 'post', 'gameplay', 'tutori']\n",
            "1\t0.49980740\tb'hey wanna check ytb channel post gameplay tutori'\n",
            "THE TWEET IS: @MangleTheLover Hey, You like FNaF? Check out our Youtube Channel! https://t.co/sc9kDhaviX :) via http://t.co/J3sxzzg7cU\n",
            "THE PROCESSED TWEET IS: ['hey', 'like', 'fnaf', 'check', 'youtub', 'channel']\n",
            "1\t0.49959948\tb'hey like fnaf check youtub channel'\n",
            "THE TWEET IS: If u like uta read the manga :-))) you'll love him even more :-))) haha yeah\n",
            "THE PROCESSED TWEET IS: ['u', 'like', 'uta', 'read', 'manga', ':-)', 'love', 'even', ':-)', 'haha', 'yeah']\n",
            "1\t0.49880783\tb'u like uta read manga :-) love even :-) haha yeah'\n",
            "THE TWEET IS: Family hang out :) ^_^ — feeling festive at Jamuna Future Park http://t.co/CXQXINIqJI\n",
            "THE PROCESSED TWEET IS: ['famili', 'hang', ':)', '—', 'feel', 'festiv', 'jamuna', 'futur', 'park']\n",
            "1\t0.49921419\tb'famili hang :)  feel festiv jamuna futur park'\n",
            "THE TWEET IS: @DeMoorSophie Hii, can you follow me, please? I'd like ask you one thing in dm :)💞\n",
            "THE PROCESSED TWEET IS: ['hii', 'follow', 'pleas', \"i'd\", 'like', 'ask', 'one', 'thing', 'dm', ':)', '💞']\n",
            "1\t0.49979977\tb\"hii follow pleas i'd like ask one thing dm :) \"\n",
            "THE TWEET IS: Back to 80s?! A brand new #Commodore64 Programming community is in town now!!!!  :) &gt;&gt;12th August @techspacekrk http://t.co/zAS2l96ljT\n",
            "THE PROCESSED TWEET IS: ['back', '80', 'brand', 'new', 'commodor', '64', 'program', 'commun', 'town', ':)', '12th', 'august']\n",
            "1\t0.49996188\tb'back 80 brand new commodor 64 program commun town :) 12th august'\n",
            "THE TWEET IS: @davidrmoloney thanks for that. You've made my day! :) Let's organise a meeting soon. Are you based in London?\n",
            "THE PROCESSED TWEET IS: ['thank', 'made', 'day', ':)', \"let'\", 'organis', 'meet', 'soon', 'base', 'london']\n",
            "1\t0.49881363\tb\"thank made day :) let' organis meet soon base london\"\n",
            "THE TWEET IS: GM \n",
            " Meet My Precious \n",
            "My Soft Lady  my Real Friend        Mizz precious :) ;):*      \n",
            "\n",
            "How Many Likes For Her No... http://t.co/G9F2cwO6WZ\n",
            "THE PROCESSED TWEET IS: ['gm', 'meet', 'preciou', 'soft', 'ladi', 'real', 'friend', 'mizz', 'preciou', ':)', ';)', 'mani', 'like', '...']\n",
            "1\t0.48767431\tb'gm meet preciou soft ladi real friend mizz preciou :) ;) mani like ...'\n",
            "THE TWEET IS: @iamAhmadshahzad aameen..:)\n",
            "Long Live Pakistan..#BleedGreen..✌🏻️\n",
            "THE PROCESSED TWEET IS: ['aameen', '..', ':)', 'long', 'live', 'pakistan', '..', 'bleedgreen', '..', '✌🏻', '️']\n",
            "1\t0.49972344\tb'aameen .. :) long live pakistan .. bleedgreen ..  '\n",
            "THE TWEET IS: Liam and Sophia in Chicago :) ♥ https://t.co/GrD53KA2G5\n",
            "THE PROCESSED TWEET IS: ['liam', 'sophia', 'chicago', ':)', '♥']\n",
            "1\t0.49855229\tb'liam sophia chicago :) '\n",
            "THE TWEET IS: Singer Dusty in TUNISIA &lt;3 Music for TUNISIA &lt;3 it was wonderful, very nice people :-) &lt;3\n",
            "\n",
            "YOUTUBE:... http://t.co/859XGmZ1W9\n",
            "THE PROCESSED TWEET IS: ['singer', 'dusti', 'tunisia', '<3', 'music', 'tunisia', '<3', 'wonder', 'nice', 'peopl', ':-)', '<3', 'youtub', '...']\n",
            "1\t0.48767431\tb'singer dusti tunisia <3 music tunisia <3 wonder nice peopl :-) <3 youtub ...'\n",
            "THE TWEET IS: That also means, imma go back to being more twitter active :D\n",
            "Cause I know everyone missed me ;) xD\n",
            "THE PROCESSED TWEET IS: ['also', 'mean', 'imma', 'go', 'back', 'twitter', 'activ', ':d', 'caus', 'know', 'everyon', 'miss', ';)', 'xd']\n",
            "1\t0.49939920\tb'also mean imma go back twitter activ :d caus know everyon miss ;) xd'\n",
            "THE TWEET IS: @TeamGtwy Thanks for adding us to your list! :) Make sure to keep in touch for more news of our light bulbs made from corn waste.\n",
            "THE PROCESSED TWEET IS: ['thank', 'ad', 'us', 'list', ':)', 'make', 'sure', 'keep', 'touch', 'news', 'light', 'bulb', 'made', 'corn', 'wast']\n",
            "1\t0.49922182\tb'thank ad us list :) make sure keep touch news light bulb made corn wast'\n",
            "THE TWEET IS: @thingforasians hi my hot girl did I say how very hot and horny you are darling xx :) ♥\n",
            "THE PROCESSED TWEET IS: ['hi', 'hot', 'girl', 'say', 'hot', 'horni', 'darl', 'xx', ':)', '♥']\n",
            "1\t0.49855229\tb'hi hot girl say hot horni darl xx :) '\n",
            "THE TWEET IS: @hinanRana WIP. Let me get a few things tweaked and I'll link you up :) Max kal tak.\n",
            "THE PROCESSED TWEET IS: ['wip', 'let', 'get', 'thing', 'tweak', \"i'll\", 'link', ':)', 'max', 'kal', 'tak']\n",
            "1\t0.49979977\tb\"wip let get thing tweak i'll link :) max kal tak\"\n",
            "THE TWEET IS: @yasminyasir96 yeah but it will be better if we use her official Account :) Like The Other @PracchiNDesai ❤️\n",
            "THE PROCESSED TWEET IS: ['yeah', 'better', 'use', 'offici', 'account', ':)', 'like', '❤', '️']\n",
            "1\t0.49972344\tb'yeah better use offici account :) like  '\n",
            "THE TWEET IS: Check this page belonging to one of our Watford Community Housing Trust enterprise cube particpants :-)... http://t.co/ZwaOnouXpo\n",
            "THE PROCESSED TWEET IS: ['check', 'page', 'belong', 'one', 'watford', 'commun', 'hous', 'trust', 'enterpris', 'cube', 'particp', ':-)', '...']\n",
            "1\t0.48767431\tb'check page belong one watford commun hous trust enterpris cube particp :-) ...'\n",
            "THE TWEET IS: @hufflepuffjimin omg thats such a sweet thing to say thank you :-) yeah i used to go to saudi arabia a lot because of my dad\n",
            "THE PROCESSED TWEET IS: ['omg', 'that', 'sweet', 'thing', 'say', 'thank', ':-)', 'yeah', 'use', 'go', 'saudi', 'arabia', 'lot', 'dad']\n",
            "1\t0.49939920\tb'omg that sweet thing say thank :-) yeah use go saudi arabia lot dad'\n",
            "THE TWEET IS: Your happiness is your responsibilty. So, don't ask others to make you happy! :) #FB\n",
            "THE PROCESSED TWEET IS: ['happi', 'responsibilti', 'ask', 'other', 'make', 'happi', ':)', 'fb']\n",
            "1\t0.49996952\tb'happi responsibilti ask other make happi :) fb'\n",
            "THE TWEET IS: mom + :) = horror movie\n",
            "THE PROCESSED TWEET IS: ['mom', ':)', 'horror', 'movi']\n",
            "1\t0.49987029\tb'mom :) horror movi'\n",
            "THE TWEET IS: @smallfastloud @dangerbayley we are down in the west country this weekend so next riding day is Tuesday :) sunny weekends all around\n",
            "THE PROCESSED TWEET IS: ['west', 'countri', 'weekend', 'next', 'ride', 'day', 'tuesday', ':)', 'sunni', 'weekend', 'around']\n",
            "1\t0.49931525\tb'west countri weekend next ride day tuesday :) sunni weekend around'\n",
            "THE TWEET IS: at first I did love you, but now I just wanna fuck, late night thinking of you until I got a nut :-) :v\n",
            "\n",
            "\"look... http://t.co/8YhLcb16Lf\n",
            "THE PROCESSED TWEET IS: ['first', 'love', 'wanna', 'fuck', 'late', 'night', 'think', 'got', 'nut', ':-)', 'v', 'look', '...']\n",
            "1\t0.48767431\tb'first love wanna fuck late night think got nut :-) v look ...'\n",
            "THE TWEET IS: 3hrs before.. :) tym to prepare\n",
            "THE PROCESSED TWEET IS: ['3hr', '..', ':)', 'tym', 'prepar']\n",
            "1\t0.49979977\tb'3hr .. :) tym prepar'\n",
            "THE TWEET IS: I think I might endit my London photos &amp; vids into a Minecon &amp; a London video :) just to look at and remember the great time. *-*\n",
            "THE PROCESSED TWEET IS: ['think', 'might', 'endit', 'london', 'photo', 'vid', 'minecon', 'london', 'video', ':)', 'look', 'rememb', 'great', 'time']\n",
            "1\t0.49442082\tb'think might endit london photo vid minecon london video :) look rememb great time'\n",
            "THE TWEET IS: @MCunleashed :D I can't sleep until I need to. If I try I just lay in bed bored\n",
            "THE PROCESSED TWEET IS: [':d', \"can't\", 'sleep', 'need', 'tri', 'lay', 'bed', 'bore']\n",
            "1\t0.49822043\tb\":d can't sleep need tri lay bed bore\"\n",
            "THE TWEET IS: \"I shouldn't b called a friend if I am not there when needed\" :) ...https://t.co/lfSZl8UbXt\n",
            "THE PROCESSED TWEET IS: ['b', 'call', 'friend', 'need', ':)', '...']\n",
            "1\t0.48767431\tb'b call friend need :) ...'\n",
            "THE TWEET IS: goodnight ! i love Luke with all my heart :-) all my love 💜\n",
            "THE PROCESSED TWEET IS: ['goodnight', 'love', 'luke', 'heart', ':-)', 'love', '💜']\n",
            "1\t0.49995425\tb'goodnight love luke heart :-) love '\n",
            "THE TWEET IS: @_Halie13_ thanks for being so great!:) ❤️😘\n",
            "THE PROCESSED TWEET IS: ['thank', 'great', ':)', '❤', '️', '😘']\n",
            "1\t0.49993135\tb'thank great :)   '\n",
            "THE TWEET IS: Save your money \n",
            "Or use them in a good way\n",
            "Money is money \n",
            "It's for better life\n",
            "And fun :)\n",
            "Have a nice day\n",
            "\n",
            "11:11\n",
            "THE PROCESSED TWEET IS: ['save', 'money', 'use', 'good', 'way', 'money', 'money', 'better', 'life', 'fun', ':)', 'nice', 'day', '11:11']\n",
            "1\t0.49942210\tb'save money use good way money money better life fun :) nice day 11:11'\n",
            "THE TWEET IS: when u find our ur friends ditched u \"cause of a lot of things\" :) WTF HAVE I DONE IM ACTUALLY A NICE PERSON\n",
            "THE PROCESSED TWEET IS: ['u', 'find', 'ur', 'friend', 'ditch', 'u', 'caus', 'lot', 'thing', ':)', 'wtf', 'done', 'im', 'actual', 'nice', 'person']\n",
            "1\t0.49989319\tb'u find ur friend ditch u caus lot thing :) wtf done im actual nice person'\n",
            "THE TWEET IS: TY again dear Eva.\n",
            "I'm totally agree with you :-)))\n",
            "@anvy2446 @4HUMANITEEs @SexyAF12 @kikbella @adasamper @RachelLFilsoof\n",
            "Keep Smiling u all\n",
            "THE PROCESSED TWEET IS: ['ty', 'dear', 'eva', \"i'm\", 'total', 'agre', ':-)', 'keep', 'smile', 'u']\n",
            "1\t0.49607238\tb\"ty dear eva i'm total agre :-) keep smile u\"\n",
            "THE TWEET IS: On another note, found this in Camden Town... :D worked to. http://t.co/MDtJGcXopb\n",
            "THE PROCESSED TWEET IS: ['anoth', 'note', 'found', 'camden', 'town', '...', ':d', 'work']\n",
            "1\t0.49663136\tb'anoth note found camden town ... :d work'\n",
            "THE TWEET IS: @AmberMarineArt Thanks for the favorite! :) Make sure to keep in touch for more news of our light bulbs made from corn waste.\n",
            "THE PROCESSED TWEET IS: ['thank', 'favorit', ':)', 'make', 'sure', 'keep', 'touch', 'news', 'light', 'bulb', 'made', 'corn', 'wast']\n",
            "1\t0.49922182\tb'thank favorit :) make sure keep touch news light bulb made corn wast'\n",
            "THE TWEET IS: Neuer Post Online :) @elfcosmetics Fixing Spray http://t.co/V0Rm1BsEIj\n",
            "THE PROCESSED TWEET IS: ['neuer', 'post', 'onlin', ':)', 'fix', 'spray']\n",
            "1\t0.49980740\tb'neuer post onlin :) fix spray'\n",
            "THE TWEET IS: @The_Quirk  and you would have to email me on (Donnae.Strydom@westerncape.gov.za) -  :-) ta --- @helenzille\n",
            "THE PROCESSED TWEET IS: ['would', 'email', 'donnae.strydom@westerncape.gov.za', ':-)', 'ta']\n",
            "1\t0.49999241\tb'would email donnae.strydom@westerncape.gov.za :-) ta'\n",
            "THE TWEET IS: @scousebabe888 Nice Holiday Honey!!!!!!!!!!!!!! :-) Kisses\n",
            "THE PROCESSED TWEET IS: ['nice', 'holiday', 'honey', ':-)', 'kiss']\n",
            "1\t0.49978450\tb'nice holiday honey :-) kiss'\n",
            "THE TWEET IS: @IsaacKirop Thank you for following us :) if you like cool new products, check out our campaign: http://t.co/qv2DwDbGI3\n",
            "THE PROCESSED TWEET IS: ['thank', 'follow', 'us', ':)', 'like', 'cool', 'new', 'product', 'check', 'campaign']\n",
            "1\t0.49998478\tb'thank follow us :) like cool new product check campaign'\n",
            "THE TWEET IS: @19strawberry66 im sure it wasnt anything we're not already used to :) Glad ur home!\n",
            "THE PROCESSED TWEET IS: ['im', 'sure', 'wasnt', 'anyth', \"we'r\", 'alreadi', 'use', ':)', 'glad', 'ur', 'home']\n",
            "1\t0.49368300\tb\"im sure wasnt anyth we'r alreadi use :) glad ur home\"\n",
            "THE TWEET IS: @dontyk if it's no use to you, I'll pick it up next time I'm down :) yeah, I'll cook in my nice big kitchen! 😍😊\n",
            "THE PROCESSED TWEET IS: ['use', \"i'll\", 'pick', 'next', 'time', \"i'm\", ':)', 'yeah', \"i'll\", 'cook', 'nice', 'big', 'kitchen', '😍', '😊']\n",
            "1\t0.49993135\tb\"use i'll pick next time i'm :) yeah i'll cook nice big kitchen  \"\n",
            "THE TWEET IS: @SwgGuy Hah.... :D\n",
            "Don't say sorry...\n",
            "THE PROCESSED TWEET IS: ['hah', '...', ':d', 'say', 'sorri', '...']\n",
            "1\t0.48767431\tb'hah ... :d say sorri ...'\n",
            "THE TWEET IS: @SabrinaKeane it's not a bad thing!!! I think we all have weird faces!!! It's rad :-) thanks though!!\n",
            "THE PROCESSED TWEET IS: ['bad', 'thing', 'think', 'weird', 'face', 'rad', ':-)', 'thank', 'though']\n",
            "1\t0.49448401\tb'bad thing think weird face rad :-) thank though'\n",
            "THE TWEET IS: @djcunningham Thanks for the favorite! :) Make sure to keep in touch for more news of our light bulbs made from corn waste.\n",
            "THE PROCESSED TWEET IS: ['thank', 'favorit', ':)', 'make', 'sure', 'keep', 'touch', 'news', 'light', 'bulb', 'made', 'corn', 'wast']\n",
            "1\t0.49922182\tb'thank favorit :) make sure keep touch news light bulb made corn wast'\n",
            "THE TWEET IS: @da_kar yep I'd been doing it without Ning it then was recommended a book and thought wow! This is it :) I think general positivity leads to\n",
            "THE PROCESSED TWEET IS: ['yep', \"i'd\", 'without', 'ning', 'recommend', 'book', 'thought', 'wow', ':)', 'think', 'gener', 'posit', 'lead']\n",
            "1\t0.49999241\tb\"yep i'd without ning recommend book thought wow :) think gener posit lead\"\n",
            "THE TWEET IS: @GlamDianee :) now, i have a reason more to dream....\n",
            "THE PROCESSED TWEET IS: [':)', 'reason', 'dream', '...']\n",
            "1\t0.48767431\tb':) reason dream ...'\n",
            "THE TWEET IS: Goodmorning. \n",
            "\n",
            "Just smile through the debit orders :) it will all soon be over.\n",
            "THE PROCESSED TWEET IS: ['goodmorn', 'smile', 'debit', 'order', ':)', 'soon']\n",
            "1\t0.49932471\tb'goodmorn smile debit order :) soon'\n",
            "THE TWEET IS: @ClimateRetweet Thanks for the retweet! :) Make sure to keep in touch for more news of our light bulbs made from corn waste.\n",
            "THE PROCESSED TWEET IS: ['thank', 'retweet', ':)', 'make', 'sure', 'keep', 'touch', 'news', 'light', 'bulb', 'made', 'corn', 'wast']\n",
            "1\t0.49922182\tb'thank retweet :) make sure keep touch news light bulb made corn wast'\n",
            "THE TWEET IS: @Niallll_1Dx it's a cool video i love it...:) thank u\n",
            "THE PROCESSED TWEET IS: ['cool', 'video', 'love', '...', ':)', 'thank', 'u']\n",
            "1\t0.49607238\tb'cool video love ... :) thank u'\n",
            "THE TWEET IS: @side556 Hey!  :)  Long time no talk...\n",
            "THE PROCESSED TWEET IS: ['hey', ':)', 'long', 'time', 'talk', '...']\n",
            "1\t0.48767431\tb'hey :) long time talk ...'\n",
            "THE TWEET IS: @realyys_ OTL NEVERMIND :( at least i got jeon so\n",
            "THE PROCESSED TWEET IS: ['otl', 'nevermind', ':(', 'least', 'got', 'jeon']\n",
            "0\t0.50000005\tb'otl nevermind :( least got jeon'\n",
            "THE TWEET IS: @blairforce2 which means its on its way over here :( 3rd load just hung up\n",
            "THE PROCESSED TWEET IS: ['mean', 'way', ':(', '3rd', 'load', 'hung']\n",
            "0\t0.50000005\tb'mean way :( 3rd load hung'\n",
            "THE TWEET IS: Seventh spot na lang :( #OTWOLGrandTrailer\n",
            "THE PROCESSED TWEET IS: ['seventh', 'spot', 'na', 'lang', ':(', 'otwolgrandtrail']\n",
            "0\t0.50016980\tb'seventh spot na lang :( otwolgrandtrail'\n",
            "THE TWEET IS: @lizzytrevaskis @1057darwin OMG! Swedish hair metal legends HäirFørce in the studio while I am on leave??? Noooo :( #givecodpieceachance\n",
            "THE PROCESSED TWEET IS: ['omg', 'swedish', 'hair', 'metal', 'legend', 'häirførc', 'studio', 'leav', 'nooo', ':(', 'givecodpieceach']\n",
            "0\t0.50000005\tb'omg swedish hair metal legend hirfrc studio leav nooo :( givecodpieceach'\n",
            "THE TWEET IS: @archietalanay dont be sad :(((((( ily\n",
            "THE PROCESSED TWEET IS: ['dont', 'sad', ':(', 'ili']\n",
            "0\t0.50055509\tb'dont sad :( ili'\n",
            "THE TWEET IS: one of my friend is following me , a little heart attack , im sorry youre blocked :((((( sadis\n",
            "THE PROCESSED TWEET IS: ['one', 'friend', 'follow', 'littl', 'heart', 'attack', 'im', 'sorri', 'your', 'block', ':(', 'sadi']\n",
            "0\t0.50000005\tb'one friend follow littl heart attack im sorri your block :( sadi'\n",
            "THE TWEET IS: @TheKelseeey awhhh ok ok :( see you nalang when class opens!!! Hehe\n",
            "THE PROCESSED TWEET IS: ['awhhh', 'ok', 'ok', ':(', 'see', 'nalang', 'class', 'open', 'hehe']\n",
            "0\t0.50148016\tb'awhhh ok ok :( see nalang class open hehe'\n",
            "THE TWEET IS: Last night was one of the worst night :( I pretty sure this Albanian women cursed me\n",
            "THE PROCESSED TWEET IS: ['last', 'night', 'one', 'worst', 'night', ':(', 'pretti', 'sure', 'albanian', 'women', 'curs']\n",
            "0\t0.50000005\tb'last night one worst night :( pretti sure albanian women curs'\n",
            "THE TWEET IS: @ChaeHyungwon_ taken :( another chara maybe? -teteh\n",
            "THE PROCESSED TWEET IS: ['taken', ':(', 'anoth', 'chara', 'mayb', 'teteh']\n",
            "0\t0.50000005\tb'taken :( anoth chara mayb teteh'\n",
            "THE TWEET IS: @ryannhough I can imagine! This would shatter my dreams :-( We'll let our @CooperativeFood colleagues know all about this. ^SB\n",
            "THE PROCESSED TWEET IS: ['imagin', 'would', 'shatter', 'dream', ':-(', \"we'll\", 'let', 'colleagu', 'know', 'sb']\n",
            "0\t0.50000005\tb\"imagin would shatter dream :-( we'll let colleagu know sb\"\n",
            "THE TWEET IS: I'm sorry I didn't see this tweet :( 2 Points x https://t.co/N6HMgKfoR1\n",
            "THE PROCESSED TWEET IS: [\"i'm\", 'sorri', 'see', 'tweet', ':(', '2', 'point', 'x']\n",
            "0\t0.50250811\tb\"i'm sorri see tweet :( 2 point x\"\n",
            "THE TWEET IS: @WeeklyChris \n",
            "\n",
            "Please tweet me something I'm sad :( you can cheer me up \n",
            "please..\n",
            "THE PROCESSED TWEET IS: ['pleas', 'tweet', 'someth', \"i'm\", 'sad', ':(', 'cheer', 'pleas', '..']\n",
            "0\t0.50135591\tb\"pleas tweet someth i'm sad :( cheer pleas ..\"\n",
            "THE TWEET IS: Alone :-( :'( :-\\\n",
            "THE PROCESSED TWEET IS: ['alon', ':-(', \":'(\", ':-\\\\']\n",
            "0\t0.50000005\tb\"alon :-( :'( :-\\\\\"\n",
            "THE TWEET IS: @rcdlccom hello, any info about possible interest in Jonathas ?? He is close to join Betis :( greatings\n",
            "THE PROCESSED TWEET IS: ['hello', 'info', 'possibl', 'interest', 'jonatha', 'close', 'join', 'beti', ':(', 'great']\n",
            "0\t0.52188837\tb'hello info possibl interest jonatha close join beti :( great'\n",
            "THE TWEET IS: @Charlie_Bread fair enough :( why would anyone do that? Just seems a tad fucked up 😔 .. It's a wiper, a wiper!\n",
            "THE PROCESSED TWEET IS: ['fair', 'enough', ':(', 'would', 'anyon', 'seem', 'tad', 'fuck', '😔', '..', 'wiper', 'wiper']\n",
            "0\t0.50000005\tb'fair enough :( would anyon seem tad fuck  .. wiper wiper'\n",
            "THE TWEET IS: Today’s job (another facking #Intel). Extra mega careful not to get bent socket pins :-( #PCGaming #PCUpgrade http://t.co/VeJNS9FBfn\n",
            "THE PROCESSED TWEET IS: ['today', '’', 'job', 'anoth', 'fack', 'intel', 'extra', 'mega', 'care', 'get', 'bent', 'socket', 'pin', ':-(', 'pcgame', 'pcupgrad']\n",
            "0\t0.50000005\tb'today  job anoth fack intel extra mega care get bent socket pin :-( pcgame pcupgrad'\n",
            "THE TWEET IS: @lukesdagger JOKE LANG EH :( HAHDHDHSHHS\n",
            "THE PROCESSED TWEET IS: ['joke', 'lang', 'eh', ':(', 'hahdhdhshh']\n",
            "0\t0.50000005\tb'joke lang eh :( hahdhdhshh'\n",
            "THE TWEET IS: Never seeing your dad until midnight bc he worked hard as fuck :( #GrowingUpPoor\n",
            "THE PROCESSED TWEET IS: ['never', 'see', 'dad', 'midnight', 'bc', 'work', 'hard', 'fuck', ':(', 'growinguppoor']\n",
            "0\t0.50000005\tb'never see dad midnight bc work hard fuck :( growinguppoor'\n",
            "THE TWEET IS: @D4nMeAtSix @carterreynolds omg why am I getting hate for being a sex offender :( poor me :( I'm a stupid pissbaby\n",
            "THE PROCESSED TWEET IS: ['omg', 'get', 'hate', 'sex', 'offend', ':(', 'poor', ':(', \"i'm\", 'stupid', 'pissbabi']\n",
            "0\t0.50000005\tb\"omg get hate sex offend :( poor :( i'm stupid pissbabi\"\n",
            "THE TWEET IS: 😂😂 what a night :(((((( so devo xxx https://t.co/9ixTnyBXLb\n",
            "THE PROCESSED TWEET IS: ['😂', '😂', 'night', ':(', 'devo', 'xxx']\n",
            "0\t0.50086406\tb'  night :( devo xxx'\n",
            "THE TWEET IS: Dadas uh iphone na, :(:( [pic] — https://t.co/Jr4U98A8ja\n",
            "THE PROCESSED TWEET IS: ['dada', 'uh', 'iphon', 'na', ':(', ':(', 'pic', '—']\n",
            "0\t0.50060272\tb'dada uh iphon na :( :( pic '\n",
            "THE TWEET IS: @OscarTrue89 no sorry I'm fully booked :-( xx\n",
            "THE PROCESSED TWEET IS: ['sorri', \"i'm\", 'fulli', 'book', ':-(', 'xx']\n",
            "0\t0.50321579\tb\"sorri i'm fulli book :-( xx\"\n",
            "THE TWEET IS: I can never go to sleep early :( lol\n",
            "THE PROCESSED TWEET IS: ['never', 'go', 'sleep', 'earli', ':(', 'lol']\n",
            "0\t0.50290866\tb'never go sleep earli :( lol'\n",
            "THE TWEET IS: @elglozano home dormtel near st scho!! all girls siya tho :( do you want me to help you find oneee\n",
            "THE PROCESSED TWEET IS: ['home', 'dormtel', 'near', 'st', 'scho', 'girl', 'siya', 'tho', ':(', 'want', 'help', 'find', 'onee']\n",
            "0\t0.50000005\tb'home dormtel near st scho girl siya tho :( want help find onee'\n",
            "THE TWEET IS: @natzaz17 Oh no, that's not good :( We're not aware of any issues. Have you been able to top-up now?\n",
            "THE PROCESSED TWEET IS: ['oh', \"that'\", 'good', ':(', \"we'r\", 'awar', 'issu', 'abl', 'top-up']\n",
            "0\t0.50000005\tb\"oh that' good :( we'r awar issu abl top-up\"\n",
            "THE TWEET IS: @iamcharleigh_ :( have fun\n",
            "THE PROCESSED TWEET IS: [':(', 'fun']\n",
            "0\t0.50447266\tb':( fun'\n",
            "THE TWEET IS: @HanaaGhzlli hanaaaa its your birthday???? ya Allah sorry for not wishing you in the van jn i tak tau :( happy birthday gorgeous!\n",
            "THE PROCESSED TWEET IS: ['hanaaa', 'birthday', 'ya', 'allah', 'sorri', 'wish', 'van', 'jn', 'tak', 'tau', ':(', 'happi', 'birthday', 'gorgeou']\n",
            "0\t0.50147253\tb'hanaaa birthday ya allah sorri wish van jn tak tau :( happi birthday gorgeou'\n",
            "THE TWEET IS: Dhis👉 @AhmedMarzooq blocked my twitter :( thank you for the good time 😢 I play 8ball,  FAKMAREY,\n",
            "THE PROCESSED TWEET IS: ['dhi', '👉', 'block', 'twitter', ':(', 'thank', 'good', 'time', '😢', 'play', '8ball', 'fakmarey']\n",
            "0\t0.50000005\tb'dhi  block twitter :( thank good time  play 8ball fakmarey'\n",
            "THE TWEET IS: @izzsugden yeah I've seen something about tinder on fb!! I was rooting for them :( but the other woman is frustrating me being so awkward!\n",
            "THE PROCESSED TWEET IS: ['yeah', \"i'v\", 'seen', 'someth', 'tinder', 'fb', 'root', ':(', 'woman', 'frustrat', 'awkward']\n",
            "0\t0.50018506\tb\"yeah i'v seen someth tinder fb root :( woman frustrat awkward\"\n",
            "THE TWEET IS: @LizaNMinnelli i can't watch it on my phone. Sorry :( thank you still &lt;3\n",
            "THE PROCESSED TWEET IS: [\"can't\", 'watch', 'phone', 'sorri', ':(', 'thank', 'still', '<3']\n",
            "0\t0.51816354\tb\"can't watch phone sorri :( thank still <3\"\n",
            "THE TWEET IS: The taxi driver thought me and Sophie worked for Nintendo and took us into the gates :( the security guards gave us the dirtiest looks haha\n",
            "THE PROCESSED TWEET IS: ['taxi', 'driver', 'thought', 'sophi', 'work', 'nintendo', 'took', 'us', 'gate', ':(', 'secur', 'guard', 'gave', 'us', 'dirtiest', 'look', 'haha']\n",
            "0\t0.50351711\tb'taxi driver thought sophi work nintendo took us gate :( secur guard gave us dirtiest look haha'\n",
            "THE TWEET IS: :((( (at Crepes 40) — https://t.co/bazEmHlhYL\n",
            "THE PROCESSED TWEET IS: [':(', 'crepe', '40', '—']\n",
            "0\t0.50060272\tb':( crepe 40 '\n",
            "THE TWEET IS: Can not get on #hypixel :( want to record grrrr #thestruggleisreal #geek #gamer #gamers #youtube\n",
            "THE PROCESSED TWEET IS: ['get', 'hypixel', ':(', 'want', 'record', 'grrr', 'thestruggleisr', 'geek', 'gamer', 'gamer', 'youtub']\n",
            "0\t0.50070958\tb'get hypixel :( want record grrr thestruggleisr geek gamer gamer youtub'\n",
            "THE TWEET IS: @GarryBrown75 That's awful :( My son missed his physics exam because he had chicken pox and they gave him a predicted grade. Poor girl.\n",
            "THE PROCESSED TWEET IS: [\"that'\", 'aw', ':(', 'son', 'miss', 'physic', 'exam', 'chicken', 'pox', 'gave', 'predict', 'grade', 'poor', 'girl']\n",
            "0\t0.50301551\tb\"that' aw :( son miss physic exam chicken pox gave predict grade poor girl\"\n",
            "THE TWEET IS: @jesuskylie @tothebeyhive thats not a good enough of a reason :( please dont leave youre one of my fav barbs\n",
            "THE PROCESSED TWEET IS: ['that', 'good', 'enough', 'reason', ':(', 'pleas', 'dont', 'leav', 'your', 'one', 'fav', 'barb']\n",
            "0\t0.50000005\tb'that good enough reason :( pleas dont leav your one fav barb'\n",
            "THE TWEET IS: :((((( matt\n",
            "THE PROCESSED TWEET IS: [':(', 'matt']\n",
            "0\t0.50073247\tb':( matt'\n",
            "THE TWEET IS: @DEPORSEMPRE1 hello, any info about possible interest of Jonathas ?? He is close to join Betis :( saludos\n",
            "THE PROCESSED TWEET IS: ['hello', 'info', 'possibl', 'interest', 'jonatha', 'close', 'join', 'beti', ':(', 'saludo']\n",
            "0\t0.50000005\tb'hello info possibl interest jonatha close join beti :( saludo'\n",
            "THE TWEET IS: @harriep Hi Harry, sorry :( We're working on a mast in the area. This can cause intermittent service until the work has been completed.\n",
            "THE PROCESSED TWEET IS: ['hi', 'harri', 'sorri', ':(', \"we'r\", 'work', 'mast', 'area', 'caus', 'intermitt', 'servic', 'work', 'complet']\n",
            "0\t0.50069431\tb\"hi harri sorri :( we'r work mast area caus intermitt servic work complet\"\n",
            "THE TWEET IS: @phenomyoutube u probs had more fun with david than me : (\n",
            "THE PROCESSED TWEET IS: ['u', 'prob', 'fun', 'david']\n",
            "0\t0.50018506\tb'u prob fun david'\n",
            "THE TWEET IS: First two days of Katies summer and she's been back and forth to the doctor with suspect meningitis and viral tonsillitis :( poorly girl 😷\n",
            "THE PROCESSED TWEET IS: ['first', 'two', 'day', 'kati', 'summer', 'back', 'forth', 'doctor', 'suspect', 'mening', 'viral', 'tonsil', ':(', 'poorli', 'girl', '😷']\n",
            "0\t0.50000005\tb'first two day kati summer back forth doctor suspect mening viral tonsil :( poorli girl '\n",
            "THE TWEET IS: @katiejosephx I've had no reply yet :(( !! But ange has given me some work for the next 3 weeks 😝 x\n",
            "THE PROCESSED TWEET IS: [\"i'v\", 'repli', 'yet', ':(', 'ang', 'given', 'work', 'next', '3', 'week', '😝', 'x']\n",
            "0\t0.50250811\tb\"i'v repli yet :( ang given work next 3 week  x\"\n",
            "THE TWEET IS: @kennyfairley @kevrobbo27 i don't think we will ever win the Petrofac Cup :-( ;-)\n",
            "THE PROCESSED TWEET IS: ['think', 'ever', 'win', 'petrofac', 'cup', ':-(', ';-)']\n",
            "0\t0.50055509\tb'think ever win petrofac cup :-( ;-)'\n",
            "THE TWEET IS: @VyenAngel gosh its cheaper in malaysia :( here its worth 130 +shipping\n",
            "THE PROCESSED TWEET IS: ['gosh', 'cheaper', 'malaysia', ':(', 'worth', '130', 'ship']\n",
            "0\t0.50052456\tb'gosh cheaper malaysia :( worth 130 ship'\n",
            "THE TWEET IS: Guys nooooo :-( what a WOW!!!!! Wow wow wow\n",
            "THE PROCESSED TWEET IS: ['guy', 'nooo', ':-(', 'wow', 'wow', 'wow', 'wow']\n",
            "0\t0.50181966\tb'guy nooo :-( wow wow wow wow'\n",
            "THE TWEET IS: @ladyliaaxoxo but never dedicating anything to me :( smh lol\n",
            "THE PROCESSED TWEET IS: ['never', 'dedic', 'anyth', ':(', 'smh', 'lol']\n",
            "0\t0.50290866\tb'never dedic anyth :( smh lol'\n",
            "THE TWEET IS: @hannardynamite I only have the morning off work unfortunately :( are you at GDCE/Gamescom?\n",
            "THE PROCESSED TWEET IS: ['morn', 'work', 'unfortun', ':(', 'gdce', 'gamescom']\n",
            "0\t0.50000005\tb'morn work unfortun :( gdce gamescom'\n",
            "THE TWEET IS: @VGO__ Oh no :( Do't think that. Do you have a parcel coming from Yodel now? If so hit follow &amp; DM over your tracking num. Chelsea\n",
            "THE PROCESSED TWEET IS: ['oh', ':(', \"do't\", 'think', 'parcel', 'come', 'yodel', 'hit', 'follow', 'dm', 'track', 'num', 'chelsea']\n",
            "0\t0.50017743\tb\"oh :( do't think parcel come yodel hit follow dm track num chelsea\"\n",
            "THE TWEET IS: Gusto ko ng Rodic's :( someone share an order with me because one's big\n",
            "THE PROCESSED TWEET IS: ['gusto', 'ko', 'ng', \"rodic'\", ':(', 'someon', 'share', 'order', \"one'\", 'big']\n",
            "0\t0.50172044\tb\"gusto ko ng rodic' :( someon share order one' big\"\n",
            "THE TWEET IS: The last few episodes were really intense :(((((( kagami n kuroko forever &lt;3\n",
            "THE PROCESSED TWEET IS: ['last', 'episod', 'realli', 'intens', ':(', 'kagami', 'n', 'kuroko', 'forev', '<3']\n",
            "0\t0.51816354\tb'last episod realli intens :( kagami n kuroko forev <3'\n",
            "THE TWEET IS: @seokielips sorry :( i spend more time on instagram now\n",
            "THE PROCESSED TWEET IS: ['sorri', ':(', 'spend', 'time', 'instagram']\n",
            "0\t0.50034718\tb'sorri :( spend time instagram'\n",
            "THE TWEET IS: Missin my homeslice on her bday :(( &lt;/333 @wraithmedia http://t.co/1J3qYUTTqG\n",
            "THE PROCESSED TWEET IS: ['missin', 'homeslic', 'bday', ':(', '</3', '33']\n",
            "0\t0.50037008\tb'missin homeslic bday :( </3 33'\n",
            "THE TWEET IS: MY FAV EMOTICON RIGHT NOW IS THE \" :(: \" EMOTICON\n",
            "THE PROCESSED TWEET IS: ['fav', 'emoticon', 'right', ':(', 'emoticon']\n",
            "0\t0.50000005\tb'fav emoticon right :( emoticon'\n",
            "THE TWEET IS: @gambl3d Hey there - sorry to hear this :( Have you checked the service status page → http://t.co/xyUrkkSa60? Gen\n",
            "THE PROCESSED TWEET IS: ['hey', 'sorri', 'hear', ':(', 'check', 'servic', 'statu', 'page', '→']\n",
            "0\t0.50018506\tb'hey sorri hear :( check servic statu page '\n",
            "THE TWEET IS: This time last week we were on route to Lovebox :( @MixedGemsBeauty shares her day and the new @CollectionLove prods http://t.co/tChmTf8om4\n",
            "THE PROCESSED TWEET IS: ['time', 'last', 'week', 'rout', 'lovebox', ':(', 'share', 'day', 'new', 'prod']\n",
            "0\t0.50000005\tb'time last week rout lovebox :( share day new prod'\n",
            "THE TWEET IS: @waniiamira ehem 😏😏 haha ala yeke :( its okay then, have fun in kk! jumpa next time, in ✈️ or 🇺🇸 maybe😋\n",
            "THE PROCESSED TWEET IS: ['ehem', '😏', '😏', 'haha', 'ala', 'yeke', ':(', 'okay', 'fun', 'kk', 'jumpa', 'next', 'time', '✈', '️', '🇺', '🇸', 'mayb', '😋']\n",
            "0\t0.50000005\tb'ehem   haha ala yeke :( okay fun kk jumpa next time     mayb '\n",
            "THE TWEET IS: @TroutAmbush I'm at work :(\n",
            "I can show you once I get home, but during 1.2 I lost my maps by accident and lost my builds\n",
            "THE PROCESSED TWEET IS: [\"i'm\", 'work', ':(', 'show', 'get', 'home', '1.2', 'lost', 'map', 'accid', 'lost', 'build']\n",
            "0\t0.50054746\tb\"i'm work :( show get home 1.2 lost map accid lost build\"\n",
            "THE TWEET IS: @remnantsofme Oh no :( What building &amp; suite are you in!?\n",
            "THE PROCESSED TWEET IS: ['oh', ':(', 'build', 'suit']\n",
            "0\t0.50035481\tb'oh :( build suit'\n",
            "THE TWEET IS: I wanna cut k but I'm with my friends and they threw me a sleepover party :( so I'm pretending to be ok like usual\n",
            "THE PROCESSED TWEET IS: ['wanna', 'cut', 'k', \"i'm\", 'friend', 'threw', 'sleepov', 'parti', ':(', \"i'm\", 'pretend', 'ok', 'like', 'usual']\n",
            "0\t0.50033955\tb\"wanna cut k i'm friend threw sleepov parti :( i'm pretend ok like usual\"\n",
            "THE TWEET IS: @ZombieHam There are a lot of good comics events but they all seem to be for kids :( [part of the Schools program]\n",
            "THE PROCESSED TWEET IS: ['lot', 'good', 'comic', 'event', 'seem', 'kid', ':(', 'part', 'school', 'program']\n",
            "0\t0.50074011\tb'lot good comic event seem kid :( part school program'\n",
            "THE TWEET IS: @deefizzy :-( poor boy\n",
            "THE PROCESSED TWEET IS: [':-(', 'poor', 'boy']\n",
            "0\t0.50047877\tb':-( poor boy'\n",
            "THE TWEET IS: crying :(\n",
            "#PDApaghimok\n",
            "THE PROCESSED TWEET IS: ['cri', ':(', 'pdapaghimok']\n",
            "0\t0.50000005\tb'cri :( pdapaghimok'\n",
            "THE TWEET IS: @pinkle_dhesi @HonouredMind no nigga in dallas looks like that :( sale sare bandar varge\n",
            "THE PROCESSED TWEET IS: ['nigga', 'dalla', 'look', 'like', ':(', 'sale', 'sare', 'bandar', 'varg']\n",
            "0\t0.50000005\tb'nigga dalla look like :( sale sare bandar varg'\n",
            "THE TWEET IS: @FreedomChild3 Had wondered why this has not happened earlier? But then I realized, we don't have the leadership to do it!  :-( #wakeupGOP\n",
            "THE PROCESSED TWEET IS: ['wonder', 'happen', 'earlier', 'realiz', 'leadership', ':-(', 'wakeupgop']\n",
            "0\t0.50000005\tb'wonder happen earlier realiz leadership :-( wakeupgop'\n",
            "THE TWEET IS: Bull shark :( I am so late!!!! Trafficccccccc\n",
            "THE PROCESSED TWEET IS: ['bull', 'shark', ':(', 'late', 'trafficcc']\n",
            "0\t0.50000005\tb'bull shark :( late trafficcc'\n",
            "THE TWEET IS: @haelic but last time the one was working :( and their fb is gone too? i haven't checked for a long time lol\n",
            "THE PROCESSED TWEET IS: ['last', 'time', 'one', 'work', ':(', 'fb', 'gone', 'check', 'long', 'time', 'lol']\n",
            "0\t0.50290866\tb'last time one work :( fb gone check long time lol'\n",
            "THE TWEET IS: @LittleMix Poland is not faraway from Germany girls :( I still believe that one day we can meet\n",
            "THE PROCESSED TWEET IS: ['poland', 'faraway', 'germani', 'girl', ':(', 'still', 'believ', 'one', 'day', 'meet']\n",
            "0\t0.50057219\tb'poland faraway germani girl :( still believ one day meet'\n",
            "THE TWEET IS: @theresaninkspot :( Well, the cookies better be worth it\n",
            "THE PROCESSED TWEET IS: [':(', 'well', 'cooki', 'better', 'worth']\n",
            "0\t0.50127988\tb':( well cooki better worth'\n",
            "THE TWEET IS: @angelicaorganic I keep forgetting you're in the area.  :-( but want to visit. Next time xx\n",
            "THE PROCESSED TWEET IS: ['keep', 'forget', 'area', ':-(', 'want', 'visit', 'next', 'time', 'xx']\n",
            "0\t0.50321579\tb'keep forget area :-( want visit next time xx'\n",
            "THE TWEET IS: @Graceaddley24 miss you too :( ! And haha the x's 😂😂 I still remember how u hate them xxxxx\n",
            "THE PROCESSED TWEET IS: ['miss', ':(', 'haha', \"x'\", '😂', '😂', 'still', 'rememb', 'u', 'hate', 'xxx']\n",
            "0\t0.50086406\tb\"miss :( haha x'   still rememb u hate xxx\"\n",
            "THE TWEET IS: @ParkSooyoungie @_allypasturan @gbrllx aytona? hala guys get ready na mathird wheel :(( jkjk\n",
            "THE PROCESSED TWEET IS: ['aytona', 'hala', 'guy', 'get', 'readi', 'na', 'mathird', 'wheel', ':(', 'jkjk']\n",
            "0\t0.50000005\tb'aytona hala guy get readi na mathird wheel :( jkjk'\n",
            "THE TWEET IS: can only backtrack 3 hours :( .... it's time to start actually utilizing these lists\n",
            "THE PROCESSED TWEET IS: ['backtrack', '3', 'hour', ':(', '...', 'time', 'start', 'actual', 'util', 'list']\n",
            "0\t0.50127225\tb'backtrack 3 hour :( ... time start actual util list'\n",
            "THE TWEET IS: pats jay : (\n",
            "THE PROCESSED TWEET IS: ['pat', 'jay']\n",
            "0\t0.50055509\tb'pat jay'\n",
            "THE TWEET IS: @londonbakes oh no :( hope everything is okay.\n",
            "THE PROCESSED TWEET IS: ['oh', ':(', 'hope', 'everyth', 'okay']\n",
            "0\t0.50014873\tb'oh :( hope everyth okay'\n",
            "THE TWEET IS: @MrsAnneTwist @Argos_Online @macmillancancer i want i want i want you to follow me i want i want i want but that's crazy :( please love!(:♡♡\n",
            "THE PROCESSED TWEET IS: ['want', 'want', 'want', 'follow', 'want', 'want', 'want', \"that'\", 'crazi', ':(', 'pleas', 'love', '(:', '♡', '♡']\n",
            "0\t0.50067905\tb\"want want want follow want want want that' crazi :( pleas love (:  \"\n",
            "THE TWEET IS: @Teeena232 We're sorry about this :( You'll no longer be able to access the site as you'll automatically be redirected to your local site.\n",
            "THE PROCESSED TWEET IS: [\"we'r\", 'sorri', ':(', 'longer', 'abl', 'access', 'site', 'automat', 'redirect', 'local', 'site']\n",
            "0\t0.50071721\tb\"we'r sorri :( longer abl access site automat redirect local site\"\n",
            "THE TWEET IS: carter: I don't deserve this hate :(\n",
            "me: and i don't deserve just a single corn chip? I deserve two corn chips?\n",
            "THE PROCESSED TWEET IS: ['carter', 'deserv', 'hate', ':(', 'deserv', 'singl', 'corn', 'chip', 'deserv', 'two', 'corn', 'chip']\n",
            "0\t0.50037008\tb'carter deserv hate :( deserv singl corn chip deserv two corn chip'\n",
            "THE TWEET IS: :( I can no longer admire the consistency of Jason Shackell's hair on the @NoNayNeverNet podcast http://t.co/T9rpuLyb5u\n",
            "THE PROCESSED TWEET IS: [':(', 'longer', 'admir', 'consist', 'jason', \"shackell'\", 'hair', 'podcast']\n",
            "0\t0.50000005\tb\":( longer admir consist jason shackell' hair podcast\"\n",
            "THE TWEET IS: I want both of this sa bday ko :( :( :( ^_^ ^_^ &lt;3\n",
            "THE PROCESSED TWEET IS: ['want', 'sa', 'bday', 'ko', ':(', ':(', ':(', '<3']\n",
            "0\t0.51816354\tb'want sa bday ko :( :( :( <3'\n",
            "THE TWEET IS: @GZTAEHUN ok :-( i only have my d &amp; w hahaahahahaha\n",
            "THE PROCESSED TWEET IS: ['ok', ':-(', 'w', 'hahaahahahaha']\n",
            "0\t0.50000005\tb'ok :-( w hahaahahahaha'\n",
            "THE TWEET IS: @JayMcGuiness  :-( please notice mefd\n",
            "THE PROCESSED TWEET IS: [':-(', 'pleas', 'notic', 'mefd']\n",
            "0\t0.50000005\tb':-( pleas notic mefd'\n",
            "THE TWEET IS: Oh am I not allowed to vote for the teen choice awards?? :(( It said I'm out of the area\n",
            "THE PROCESSED TWEET IS: ['oh', 'allow', 'vote', 'teen', 'choic', 'award', ':(', 'said', \"i'm\", 'area']\n",
            "0\t0.50054746\tb\"oh allow vote teen choic award :( said i'm area\"\n",
            "THE TWEET IS: @JayMcGuiness  :-( please notice meowkd\n",
            "THE PROCESSED TWEET IS: [':-(', 'pleas', 'notic', 'meowkd']\n",
            "0\t0.50000005\tb':-( pleas notic meowkd'\n",
            "THE TWEET IS: @Haydenwellsss @JoshWalker93 noo sad times :( Ruby will have to replace you as the shuffling act\n",
            "THE PROCESSED TWEET IS: ['noo', 'sad', 'time', ':(', 'rubi', 'replac', 'shuffl', 'act']\n",
            "0\t0.50072484\tb'noo sad time :( rubi replac shuffl act'\n",
            "THE TWEET IS: @AaronCarpenter is it too late? :( #Followmeaaron\n",
            "THE PROCESSED TWEET IS: ['late', ':(', 'followmeaaron']\n",
            "0\t0.50000005\tb'late :( followmeaaron'\n",
            "THE TWEET IS: @JayMcGuiness  :-( please notice mew\n",
            "THE PROCESSED TWEET IS: [':-(', 'pleas', 'notic', 'mew']\n",
            "0\t0.50000005\tb':-( pleas notic mew'\n",
            "THE TWEET IS: some of the best spectator sailing UK this w'end but BBC just showing highlights :( ‘flying’ foiling AC45 catamarans http://t.co/p4n34Djjuw\n",
            "THE PROCESSED TWEET IS: ['best', 'spectat', 'sail', 'uk', \"w'end\", 'bbc', 'show', 'highlight', ':(', '‘', 'fli', '’', 'foil', 'ac45', 'catamaran']\n",
            "0\t0.50000005\tb\"best spectat sail uk w'end bbc show highlight :(  fli  foil ac45 catamaran\"\n",
            "THE TWEET IS: penge bestfriend :( #PSYGustoKita\n",
            "THE PROCESSED TWEET IS: ['peng', 'bestfriend', ':(', 'psygustokita']\n",
            "0\t0.50054746\tb'peng bestfriend :( psygustokita'\n",
            "THE TWEET IS: that's kind of a dumb statement because who LIKES moodswings?? it's like making a tweet saying \"I hate terminal diseases :(\" like wow unique\n",
            "THE PROCESSED TWEET IS: [\"that'\", 'kind', 'dumb', 'statement', 'like', 'moodsw', 'like', 'make', 'tweet', 'say', 'hate', 'termin', 'diseas', ':(', 'like', 'wow', 'uniqu']\n",
            "0\t0.50017743\tb\"that' kind dumb statement like moodsw like make tweet say hate termin diseas :( like wow uniqu\"\n",
            "THE TWEET IS: @JayMcGuiness @JayMcGuiness :-( please notice men\n",
            "THE PROCESSED TWEET IS: [':-(', 'pleas', 'notic', 'men']\n",
            "0\t0.50148016\tb':-( pleas notic men'\n",
            "THE TWEET IS: @j13ssk Hi there, oh no :( Let's take a look at this for you, please chat to us here http://t.co/otFGtFoeyF\n",
            "THE PROCESSED TWEET IS: ['hi', 'oh', ':(', \"let'\", 'take', 'look', 'pleas', 'chat', 'us']\n",
            "0\t0.51047752\tb\"hi oh :( let' take look pleas chat us\"\n",
            "THE TWEET IS: @JayMcGuiness @JayMcGuiness :-( please notice mef\n",
            "THE PROCESSED TWEET IS: [':-(', 'pleas', 'notic', 'mef']\n",
            "0\t0.50000005\tb':-( pleas notic mef'\n",
            "THE TWEET IS: @CllrPeterSarris :-( @ missing all the fun\n",
            "THE PROCESSED TWEET IS: [':-(', 'miss', 'fun']\n",
            "0\t0.50447266\tb':-( miss fun'\n",
            "THE TWEET IS: Naw :( Deep Dreamed my novel hoping there'd be invisible page beasts scuttling around but there's just worm tracks http://t.co/feTHZT8bfs\n",
            "THE PROCESSED TWEET IS: ['naw', ':(', 'deep', 'dream', 'novel', 'hope', \"there'd\", 'invis', 'page', 'beast', 'scuttl', 'around', \"there'\", 'worm', 'track']\n",
            "0\t0.50073247\tb\"naw :( deep dream novel hope there'd invis page beast scuttl around there' worm track\"\n",
            "THE TWEET IS: She lost us, her friends... :( she's the one who started the argument.\n",
            "THE PROCESSED TWEET IS: ['lost', 'us', 'friend', '...', ':(', 'one', 'start', 'argument']\n",
            "0\t0.50000005\tb'lost us friend ... :( one start argument'\n",
            "THE TWEET IS: last night was so good :( 😺💒💎🎉\n",
            "THE PROCESSED TWEET IS: ['last', 'night', 'good', ':(', '😺', '💒', '💎', '🎉']\n",
            "0\t0.50054746\tb'last night good :(    '\n",
            "THE TWEET IS: my beloved grandmother : ( https://t.co/wt4oXq5xCf\n",
            "THE PROCESSED TWEET IS: ['belov', 'grandmoth']\n",
            "0\t0.50000005\tb'belov grandmoth'\n",
            "THE TWEET IS: I am not a happy Princess today :( sighs \n",
            "But I will get some work done \n",
            "X\n",
            "~ Purple Princess Edits ~\n",
            "THE PROCESSED TWEET IS: ['happi', 'princess', 'today', ':(', 'sigh', 'get', 'work', 'done', 'x', 'purpl', 'princess', 'edit']\n",
            "0\t0.50033955\tb'happi princess today :( sigh get work done x purpl princess edit'\n",
            "THE TWEET IS: Babeeeee :((( you're so demn hotaisndonwyvauwjoqhsjsnaihsuswtf https://t.co/kWwv5Grny7\n",
            "THE PROCESSED TWEET IS: ['babee', ':(', 'demn', 'hotaisndonwyvauwjoqhsjsnaihsuswtf']\n",
            "0\t0.50000005\tb'babee :( demn hotaisndonwyvauwjoqhsjsnaihsuswtf'\n",
            "THE TWEET IS: Nirame ya geng :( (with fikri, Anna, and 6 others at Tirtagangga Hotel) — https://t.co/46fL7vHiL3\n",
            "THE PROCESSED TWEET IS: ['niram', 'ya', 'geng', ':(', 'fikri', 'anna', '6', 'other', 'tirtagangga', 'hotel', '—']\n",
            "0\t0.50060272\tb'niram ya geng :( fikri anna 6 other tirtagangga hotel '\n",
            "THE TWEET IS: @The5BallOver @Radio702 :-( It's not a challenge though. Please check our FB page for entries and rather do a substitution. Thanks!\n",
            "THE PROCESSED TWEET IS: [':-(', 'challeng', 'though', 'pleas', 'check', 'fb', 'page', 'entri', 'rather', 'substitut', 'thank']\n",
            "0\t0.57457827\tb':-( challeng though pleas check fb page entri rather substitut thank'\n",
            "THE TWEET IS: @CHEDA_KHAN Thats life. I get calls from people I havent seen in 20 years and its always favours : (\n",
            "THE PROCESSED TWEET IS: ['that', 'life', 'get', 'call', 'peopl', 'havent', 'seen', '20', 'year', 'alway', 'favour']\n",
            "0\t0.50018506\tb'that life get call peopl havent seen 20 year alway favour'\n",
            "THE TWEET IS: #letsFootball #atk greymind43: BREAKING NEWS: Chris Gayle says he will be out of cricket for 2-3 months due to back surgery. :( #CPLT20 #CP…\n",
            "THE PROCESSED TWEET IS: ['letsfootbal', 'atk', 'greymind', '43', 'break', 'news', 'chri', 'gayl', 'say', 'cricket', '2-3', 'month', 'due', 'back', 'surgeri', ':(', 'cplt', '20', 'cp', '…']\n",
            "0\t0.50303841\tb'letsfootbal atk greymind 43 break news chri gayl say cricket 2-3 month due back surgeri :( cplt 20 cp '\n",
            "THE TWEET IS: @DeviousLiz awh :-( 💙 but you have to stay 💪\n",
            "THE PROCESSED TWEET IS: ['awh', ':-(', '💙', 'stay', '💪']\n",
            "0\t0.50018506\tb'awh :-(  stay '\n",
            "THE TWEET IS: Last full night in Greece :( @ OPUS Inner Pleasure https://t.co/poglAQg9sJ\n",
            "THE PROCESSED TWEET IS: ['last', 'full', 'night', 'greec', ':(', 'opu', 'inner', 'pleasur']\n",
            "0\t0.50185019\tb'last full night greec :( opu inner pleasur'\n",
            "THE TWEET IS: @Bobwilson1955 yes :( but we have a BBQ to attend\n",
            "THE PROCESSED TWEET IS: ['ye', ':(', 'bbq', 'attend']\n",
            "0\t0.50017743\tb'ye :( bbq attend'\n",
            "THE TWEET IS: @2baconil \n",
            "Yes many times :(\n",
            ".#QuitKarwaoYaaro\n",
            "THE PROCESSED TWEET IS: ['ye', 'mani', 'time', ':(', 'quitkarwaoyaaro']\n",
            "0\t0.50000005\tb'ye mani time :( quitkarwaoyaaro'\n",
            "THE TWEET IS: @JabongIndia very disappointed :( never ever been lucky from your handle. #JabongatPumaUrbanStampede\n",
            "THE PROCESSED TWEET IS: ['disappoint', ':(', 'never', 'ever', 'lucki', 'handl', 'jabongatpumaurbanstamped']\n",
            "0\t0.50054746\tb'disappoint :( never ever lucki handl jabongatpumaurbanstamped'\n",
            "THE TWEET IS: Should i deactivate my acc? :( #OTWOLGrandTrailer\n",
            "THE PROCESSED TWEET IS: ['deactiv', 'acc', ':(', 'otwolgrandtrail']\n",
            "0\t0.50016980\tb'deactiv acc :( otwolgrandtrail'\n",
            "THE TWEET IS: @eyeammarkk omg :( not gonna happen , yeah we wish..\n",
            "THE PROCESSED TWEET IS: ['omg', ':(', 'gonna', 'happen', 'yeah', 'wish', '..']\n",
            "0\t0.50135591\tb'omg :( gonna happen yeah wish ..'\n",
            "THE TWEET IS: @nambooti aww :( catch them at the departure then!\n",
            "THE PROCESSED TWEET IS: ['aww', ':(', 'catch', 'departur']\n",
            "0\t0.50000005\tb'aww :( catch departur'\n",
            "THE TWEET IS: @oIiverfelicitys UGH YUCK :( make sure you rest the next two days then\n",
            "THE PROCESSED TWEET IS: ['ugh', 'yuck', ':(', 'make', 'sure', 'rest', 'next', 'two', 'day']\n",
            "0\t0.51263335\tb'ugh yuck :( make sure rest next two day'\n",
            "THE TWEET IS: @mari_suunn dude :( I'm scared lol\n",
            "THE PROCESSED TWEET IS: ['dude', ':(', \"i'm\", 'scare', 'lol']\n",
            "0\t0.50290866\tb\"dude :( i'm scare lol\"\n",
            "THE TWEET IS: Almost done with parks and rec :( lol\n",
            "THE PROCESSED TWEET IS: ['almost', 'done', 'park', 'rec', ':(', 'lol']\n",
            "0\t0.50290866\tb'almost done park rec :( lol'\n",
            "THE TWEET IS: @lplatten Hi there, we're sorry to hear this :( Log in here: http://t.co/76lov3ArFc you'll be able to see all the information you need.\n",
            "THE PROCESSED TWEET IS: ['hi', \"we'r\", 'sorri', 'hear', ':(', 'log']\n",
            "0\t0.50036244\tb\"hi we'r sorri hear :( log\"\n",
            "THE TWEET IS: @guy_interruptd Aw :( *strokes*\n",
            "THE PROCESSED TWEET IS: ['aw', ':(', 'stroke']\n",
            "0\t0.50018506\tb'aw :( stroke'\n",
            "THE TWEET IS: tagged by @asabianglala ♡ i'm sorry i have A LOT of music so you prolly don't know half of it :( also @SaikyoRyouT http://t.co/AcXikXwfcI\n",
            "THE PROCESSED TWEET IS: ['tag', '♡', \"i'm\", 'sorri', 'lot', 'music', 'prolli', 'know', 'half', ':(', 'also']\n",
            "0\t0.50190545\tb\"tag  i'm sorri lot music prolli know half :( also\"\n",
            "THE TWEET IS: @GeorgiaAnne1998 Aww I've already left!! I would have come and said hey if I had have seen it sooner :( Have a good night! X\n",
            "THE PROCESSED TWEET IS: ['aww', \"i'v\", 'alreadi', 'left', 'would', 'come', 'said', 'hey', 'seen', 'sooner', ':(', 'good', 'night', 'x']\n",
            "0\t0.50250811\tb\"aww i'v alreadi left would come said hey seen sooner :( good night x\"\n",
            "THE TWEET IS: I care about Maggie but she is a psycho bitch I want to kill myself and take a two day vacation off twitter :( wahhh\n",
            "THE PROCESSED TWEET IS: ['care', 'maggi', 'psycho', 'bitch', 'want', 'kill', 'take', 'two', 'day', 'vacat', 'twitter', ':(', 'wahhh']\n",
            "0\t0.50000005\tb'care maggi psycho bitch want kill take two day vacat twitter :( wahhh'\n",
            "THE TWEET IS: Going home blues this am :-( but back on MONDAY #hibye for social action plans @Herts1617 #ShareYourSummer http://t.co/7pJJ9q5v7z\n",
            "THE PROCESSED TWEET IS: ['go', 'home', 'blue', ':-(', 'back', 'monday', 'hiby', 'social', 'action', 'plan', 'shareyoursumm']\n",
            "0\t0.50000005\tb'go home blue :-( back monday hiby social action plan shareyoursumm'\n",
            "THE TWEET IS: @shellbryson We're sorry you feel this way Shell :( We regularly review our pricing to offer the best we can.\n",
            "THE PROCESSED TWEET IS: [\"we'r\", 'sorri', 'feel', 'way', 'shell', ':(', 'regularli', 'review', 'price', 'offer', 'best']\n",
            "0\t0.50598314\tb\"we'r sorri feel way shell :( regularli review price offer best\"\n",
            "THE TWEET IS: Pengen boxing :( (at @golds_indonesia) — https://t.co/qXG4UNA4Fn\n",
            "THE PROCESSED TWEET IS: ['pengen', 'box', ':(', '—']\n",
            "0\t0.50060272\tb'pengen box :( '\n",
            "THE TWEET IS: Hulk Hogan in the news for a racial tirade :( I thought he couldnt stoop lower than his role in Thunder in Paradise http://t.co/vd0z6x8t8g\n",
            "THE PROCESSED TWEET IS: ['hulk', 'hogan', 'news', 'racial', 'tirad', ':(', 'thought', 'couldnt', 'stoop', 'lower', 'role', 'thunder', 'paradis']\n",
            "0\t0.50018506\tb'hulk hogan news racial tirad :( thought couldnt stoop lower role thunder paradis'\n",
            "THE TWEET IS: @esp__132 oh really :( i saw a few gif posts and some had seen really happy about it, but maybe it's people who'd be happy with any naruhina\n",
            "THE PROCESSED TWEET IS: ['oh', 'realli', ':(', 'saw', 'gif', 'post', 'seen', 'realli', 'happi', 'mayb', 'peopl', \"who'd\", 'happi', 'naruhina']\n",
            "0\t0.50000005\tb\"oh realli :( saw gif post seen realli happi mayb peopl who'd happi naruhina\"\n",
            "THE TWEET IS: Minho  still injured  ,, :( ,, get well Oppa \n",
            "we miss you &lt;3\n",
            "THE PROCESSED TWEET IS: ['minho', 'still', 'injur', ':(', 'get', 'well', 'oppa', 'miss', '<3']\n",
            "0\t0.51816354\tb'minho still injur :( get well oppa miss <3'\n",
            "THE TWEET IS: @Cramdaline It just doesn't go away... you must go to the dentist. :( #poorKid\n",
            "THE PROCESSED TWEET IS: ['go', 'away', '...', 'must', 'go', 'dentist', ':(', 'poorkid']\n",
            "0\t0.50000005\tb'go away ... must go dentist :( poorkid'\n",
            "THE TWEET IS: @CourtneyR_Green awwwww my baby :( 💕\n",
            "THE PROCESSED TWEET IS: ['awww', 'babi', ':(', '💕']\n",
            "0\t0.50088696\tb'awww babi :( '\n",
            "THE TWEET IS: @UyyChristopher :( Broken hearted? That's just a phase. Don't worry, you can do it!!\n",
            "THE PROCESSED TWEET IS: [':(', 'broken', 'heart', \"that'\", 'phase', 'worri']\n",
            "0\t0.50218205\tb\":( broken heart that' phase worri\"\n",
            "THE TWEET IS: @Coles45Coles \n",
            "\n",
            "wont be in work today, feel so ill :( i have a tummy bug xx\n",
            "THE PROCESSED TWEET IS: ['wont', 'work', 'today', 'feel', 'ill', ':(', 'tummi', 'bug', 'xx']\n",
            "0\t0.50321579\tb'wont work today feel ill :( tummi bug xx'\n",
            "THE TWEET IS: @MTNza seriously this is unfair some are not asking questions! There are ready to answer and win :( ='(  while we busy asking questions!!\n",
            "THE PROCESSED TWEET IS: ['serious', 'unfair', 'ask', 'question', 'readi', 'answer', 'win', ':(', \"='(\", 'busi', 'ask', 'question']\n",
            "0\t0.50181203\tb\"serious unfair ask question readi answer win :( ='( busi ask question\"\n",
            "THE TWEET IS: @DollyyDaydreams I miss you so much more :( xxx\n",
            "THE PROCESSED TWEET IS: ['miss', 'much', ':(', 'xxx']\n",
            "0\t0.50086406\tb'miss much :( xxx'\n",
            "THE TWEET IS: @ddlovato @Vevo bad :( I don't like it. This video is  too many perverse.\n",
            "THE PROCESSED TWEET IS: ['bad', ':(', 'like', 'video', 'mani', 'pervers']\n",
            "0\t0.50000005\tb'bad :( like video mani pervers'\n",
            "THE TWEET IS: Hmmm 10 mins to get my train and I'm currently about 15 mins away :( #failsatlife\n",
            "THE PROCESSED TWEET IS: ['hmmm', '10', 'min', 'get', 'train', \"i'm\", 'current', '15', 'min', 'away', ':(', 'failsatlif']\n",
            "0\t0.50000005\tb\"hmmm 10 min get train i'm current 15 min away :( failsatlif\"\n",
            "THE TWEET IS: ill be on soon, I PROMISE :(\n",
            "waaah\n",
            "THE PROCESSED TWEET IS: ['ill', 'soon', 'promis', ':(', 'waaah']\n",
            "0\t0.50000005\tb'ill soon promis :( waaah'\n"
          ]
        }
      ],
      "source": [
        "# Some error analysis done for you\n",
        "print('Label Predicted Tweet')\n",
        "for x,y in zip(test_x,test_y):\n",
        "    y_hat = predict_tweet(x, freqs, theta)\n",
        "\n",
        "    if np.abs(y - (y_hat > 0.5)) > 0:\n",
        "        print('THE TWEET IS:', x)\n",
        "        print('THE PROCESSED TWEET IS:', process_tweet(x))\n",
        "        print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ9PP7nM7SJJ"
      },
      "source": [
        "Later in this specialization, we will see how we can use deep learning to improve the prediction performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0s48ZVh7SJJ"
      },
      "source": [
        "# Part 6: Predict with your own tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1arF3O97SJJ",
        "outputId": "e12adfe1-46fc-49d1-ab30-ba5ceb782273"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ridicul', 'bright', 'movi', 'plot', 'terribl', 'sad', 'end']\n",
            "[[0.49547005]]\n",
            "Negative sentiment\n"
          ]
        }
      ],
      "source": [
        "# Feel free to change the tweet below\n",
        "my_tweet = 'This is a ridiculously bright movie. The plot was terrible and I was sad until the ending!'\n",
        "print(process_tweet(my_tweet))\n",
        "y_hat = predict_tweet(my_tweet, freqs, theta)\n",
        "print(y_hat)\n",
        "if y_hat > 0.5:\n",
        "    print('Positive sentiment')\n",
        "else: \n",
        "    print('Negative sentiment')"
      ]
    }
  ],
  "metadata": {
    "coursera": {
      "schema_names": [
        "NLPC1-1"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}